===========================EC2==============
-- elastic compute cloud

-- ec2 is specific at region only 

-- the datacentre handle by us is called "on-premises"

-- in aws L.B never down coz it is not a server it is a service from a AWS 

-- ELB is regional level

-- u can use this ELB through url / dns 

-- in generl we dont have any control on servers , but we have full control on servers which is created by ElasticBeanStalk (platform as a Service)

----------------- "LightSail" :if u want to setup and crete virtual server which is already has everything installed (Readymade )

-- it doesn't support auto-Scaling

------------------------------------------AWS LAMBDA 

-- lambda is serveless - no need to maintain servers by u 

-- we create functions in the lambda , this function is called lambda functions 

-----EVENT BRIDGE : whenever u click somethning event is generated everytime 

-- event bridge : it holds all the events of aws services  

-- u jst invoke the lambda funcn with the event bridge and do apply ur fuctions 

-- Lambda is used for the "automation"

-- lambda and event bridge are service 

-- lambda is regional level and it supports multiple languages like java,py,go,ruby 


===========================AWS - Storage S3,EBS=======================

-- in olden days 

we have Floppy -- 2MB

-- CD's = 700mb
-- DVD == 4.7GB
-- Pendrive == 12gb
-- Hard Disk == 2TB 


--- aws came with S3 (simple storage Service), it has unlimited storage 

--- u can store any type of files , u can upload and download the files

--- it is "Server less "

--- s3 --> Bucket--> Objects

-- bucket is collection of objects 

-- objects is a file

-- name of the file is called "key"

-- S3 is "Objected Based Storage"

-- it also provided "static host website"

-- s3 is global and buckets are regional

-------------------EBS

-- default volume is created when instance is created

-- default for windows ec2 instance  in aws == it has storage capacity has 30GB 

-- for Linnux ec2 instace based on os == 8 r 10 GB

-- ebs is centralized storage , u go n create volume and then attach th e intance 

-- volumes can be attached and dettach 

-- s3 is object storage and ebs is block type storage

-- default volm is called "root volume" , root volume contains o.s(linux, windows)

-- u can attach multiple volumes to the ec2 instances

-- ec2 supports only server os, not client os 

-- max capacity of ebs volume is  is 16TB

-- volumes can be pre provision

-- u cant attach a single volume to multiple ec2 at the same time 

-- volm size can be increased on FLY(no need to stop the ec2 instances)

-- volum size cant decrease once u created , so u need to delete the volume n create new volume n migrate ur data to new volm n delete the old volume .

-- Root volm has a device name for win/lin == /dev/sda1 or /dev/xvda

-- for ubuntu == /dev/xvda

-- u need to stop instance to deattach the root volm

-- it is poosible to deattach the additional volume ut it is not recommended

-- u cant delete the volm whilw it is attach so deattach first 

-- ec2 instace and EBS is should be in same A.Z 

====================================EFS=====================

-- elastic file system , it is file based storage system, it has unlimited storage

-- efs is only for the linux instances only

-- duplicate is not possible (same name with 2 files)

-- for windows ec2 instances we used == Fsx 

-- EFS works with "NFSv4" protocal 

-- in EFS u no need to pre-provision , it will automaticaaly increase and decrese based on ur file 

-- it is used to mounted to multiple ec2 instances across A.Z

-- it is regional limit

============================SNOW FAMILY===============

-- SnowCone --> 8TB

-- SnowEdge--> 100TB

-- SnowMobile --> PB's

-- it is used for physical data transfer 

-- all these snow family data  will be pushed to S3 

-- S.F is used to transfer the huge data from on-premises to AWS and vice-versa

-- it is encrypted and very safe to transfer 

------------------- Glacier 

-- it is used for "Archiving Purpose"

-- it chaper then the s3

--  all ur infrequent data stored in the Glacier

-- ----------------storage Gateway

-- it works as hybrid cloud service 

-- the data that u have in S3, ebs,fsx,glacier  move these data into the on-premises through the "storage gateway", it it support only "S3, ebs,fsx,glacier" 

----------------Database services

-- RDs -- relational database service , it is not a database it is database service 

-- all issue will be take care by the AWS 

-- rds db instance, rds supports only rdbms db only  

-- rds is a service where u can setup, configure, manage and maintain databases in aws 


-- RDS supports 6 Engines ----- MOMPMA

-- Mysql = Opensource
-- Oracel = oracle
-- Mssql = Microsoft
-- Postgresql = opensource
-- MariaDb = community based
-- Aurora = AWS 


-- if u want to transfer ur database u can use the service called "Database Migratin Service" (from on-premises to aws)


---- AWS has introduce its own NOSQL database called "DynamoDB"

-- it is nosql database service in aws 

--  if u are storing huge data it is called " DatawareHouse"

-- the dataware house service in the aws is called "Redshift" 

-- if u have frequently data then db server has stored in the Cached Memory 

-- in aws cached memory service is called "Elastic cache" which is in-memory database service

-- low latency

-- high performance

-- "Elastic cache" supports 2 engines 

1 Redis 

2 Memcached 

-- these are databses only but these are cache databases


===================================Route53============

-- it is DNS service from AWS 

-- dns pot number is 53 , so it is called r53

-- it contians records 

--- it is "Global"

-------------VPC 

-- whatever resource that we have created all these are inside VPC only, 

-- it is regional limit

-- 5 vpc per region we are able to create

-- this is we can called as "virtual Datacenter on AWS/cloud

-- by deault 2 vpc cant talk each other, but if it is required we can 

-- aws provided default vpc for us 

-- every region has a default vpc

-- VPN : virtual private network , in the shared internet we use vpn for secure purpose 

-- Direct Connect : if u want to connect directly to aws without shared internet , it is quite expensive around 16k/month


---- if u have customers all over the world , but ur appn in mumbai, the soln is to avoid high latency to the customers through out the world u can use "CloudFront" service


-- in C.F we have Edge Locations ,every regions have edge locations and all these are cache ur application

-- in C.F u create "Distribution" , while creating distribution u have to give "Origin"(elb,s3,Elasticbeanstalk..etc ) and TTL Value

-- then u have to specify , which continent aws will cache ur appn(us,europe,asia) and geo-restriction also apply if u dont want to cache in specific continents

-- how does cache ur appn ?

by using CDN = Content Delivery Network , by using this network appn is cached in the edge location 

-- how much time it will get Cache ?

ans: TTL(time to live) , if u give ttl is 5 hours then it will cache for 5 hours 

---- IMP: once u want to change the file , then it wont effect immediatly on edge-location , it will reflect only fter 5 hrs only

-- so if u want to update the data immediatly in the edge locations we have concept called "invalidate Cache" 

-- which is used to force the update content in the edge loction

-- all the data cached based on TTL

IMP : C.F will cache Static and Dynamic content in the edge locations

--  c.f has edge locatins and these are connected with CDN



==============================IAM========================

-- Identity and access management 

-- in the root account(main account) -- root user and IAM account 

-- inside the root accnt we will create users called "iam user"

-- permissions = policies both are same 

-- u can control the entire AWS using IAM by providing proper pemissions to the IAM user

-- two types of accounts 

1 ROOT USER  -- super user have all permissions == log with Email/pwd

2 IAM user --- limited permissions == login with username/pwd

--- IAM is Access control on AWS Resource

----------------------Organizations

--- it will help to manage multiple root accounts 

-- in organzations will have member accounts 

-- u can control member accounts using SCP (service Control Policies)

---------CloudWatch 

-- used to monitor AWS services  

-- u can set alarm using metrics(cpu,memory,network,requests etc...) in Cloud watch to send notifications through SNS

-- c.w monitors the performance

-- they are 2 types f mnitoring 

1 basic monitoring : u will get data points every 5 min , which is free

2 Detailed Monitoring : u will get datat points every 1 min, not free


--- with c.w u can monitor appn also 

-----------CloudTrail 

-- whatever is happening in the aws console , all these are recorded in the c.T, it monitors entire AWS environment 

-- it records, monitors, tracks, auditing, logs etc....

------------Config 

-- it is used to monitors the changes in AWS Resource 


-------------------AWS Support 

-- we have different supports 

1 basic suppport  : FREEE 

2 Developer support : 100$

3 bussiness support : this is paid service , this is meant for enterprise level

-- u will get reply within an hour 

4 enterprise support   : a paid support service with the highest SLA available in 15 minutes 

-- in this support u wil get one TAM(technical Account Manager) who are subject matter experts in their own fields 

$15000 


--- these are all based on SLA (service level Aggrement) 




==================================IAM -- depth 

-- with iam u cn conrol aws centrally 

-- IAM is "Global"

-- with the iam user/root user details u cant login into the EC2 instances but u can access ec2 service

-- with the iam user/root user details used to jst login into the aws console 

-- it is not ecommended to use root account for dailt work , instead of that use IAM user 

-- to provide more security for ur account enable MFA (multi factor authentication) like google authenticator 

-- MFA is ighly recommended for ROOT and IAM user account as well 

-- 2 ways to access aws 

1 AWS Console (GUI)

2 programmatical Access : CLI, SDK's ,developer tools , it dont have any MFA

-- every IAM user can have max 2 set of keys 

-- dont create keys for root accnt

=================================IAM Groups=========

-- colectin of IAM users

-- group under groups are not possibles

-- it is possible to attach multiple policies to the IAM user, max is 10 policies

-- u cant assign or create kys to the groups

-- an IAM user can be attached to the multiple groups at the same time

-- policy documents contain permissions

-- policies are written in "JSON" format

-- they are 2 types of policies

1 manage policy : pre-defined policies created and managed by AWS 

2 inline policy : created and mnaged by the customers

-- ARN : Amazon Resource Name : if u want to give permissions to certain group or user by the id (ARN)

-------------------------IAM ROLES

--  Temporary Access without credentials

-- u need to attach the roles to the ec2 instance in the backend 

-- if u use th roles, u no need to configure keys on the ec2 instance

-- based on the permissions that you attached to the ROLE, those pemissions are availabl from the ec2 instance

-- 1 ec2 instance can have only 1 ROLE attached

-- 1 ROLE can be attached to multiple ec2 instance

----------- Identity Povider / Federation / IAM Identity Center

-- SSO = Single Sign On 


-- ACTIVE DIRECTORY also know as "Domain Controller"

-- these are Directory Services


-- to setup sso in AWS 

-- Feeration to from LDAP to AWS 


====================================IAM TAGS=======================

-- Tags are key vaue pair 

-- tags are used for identification purpose

-- for every resource u have tags

-- ARN are generated by AWS , where as tags are providede by Customer

-- Tags are used for automation purpose

-- Tags are used for cost optimization purpose also

-- u can give "50 tags" max per resource

-- tags are imp but these are optional

-- Access Advisior : usd for auditing purpose , it shows the srvice permissions granted to the user and when those were last used 

-- it is usd to revise ur policies to the users


-- Access Analyzer : it is used to analyze the access for all IAM users and take actions on the findings




IMP : to access he other accounts , by using ROLES u can access others account with the required permissions , once u set the policies on ur account and create role it wi create one URL and these url u can share it to the others to acces ur account with the given policies through the Roles 

-- Always use inline policy for the Roles



===========================IAM PRACTICALS=======================

--create alias for ur account 

-- open IAM in console nd create Alias name for ur account, instead of remember the account number u jst create alias name 


-- explore allthings in IAM 


==========================IAM Policies


-- Inline policy 

eg: how to cretae groups and users only 


-- Policies > create policy > visual editor > select Service > slect acccoding to rquiremts > injso automatically it will create jso code > c.o nxt to create policy 



=================Roles

--  without credentials we can acess ervice for certain period

-- go to roles > aws service > ec2 > select permision 


==========identit center 

eg :if someone want to access 3 aws accounts u no need to create iam user in 3 aws accounts , u jst go to I.C and create user name and give URL to the person he will able to login with 3 aws accounts with SSO(single sign one ) as they wants to login into which account with same credentils 


========================ec2

--it is  a web sevice from aws tht provides resizable compute in the cloud

-- resizable: scale in n scale out == elasticity

-- scaleup n scale in == scalability 

-- ec2 is regional 

-- ---------Pricing models in AWS

1  ON-Demand instances: pay as u go 

-fixed price , pay per hour 
- no upfront payments
- no predictable
- no commitment
- short term committment


2  Reserved instances 

- Long term commitent 
- 1 or 3 years
- upfront payments(full , partial)
- 75% discount approx

------- we have 3 types of RI

a   Standard RI : where u get 75% discount

b   convertible RI : to change the capacity of the instance 66% discount

c   Schedule RI :  reserve it for short term like fraction of day , week, or month


3 Spot Instances: 

- Biding 
- huge capacity for cheaper price
- 90% discount


4 Dedicated Host

- if u need a physical machine with VM's for this model 

- privaacy

- high security

---- ---------recently aws launched savings plans also 

- it has same as RI but difernet starategy


----------------------------------------------EC2Families/instance type 

- General Instance = for general purpose

- Memory instannce = if u need more memory for ur application

- CPU instances = more CPU's

- storage = for storage purpose

- GPU = for heavy machines, Graphicd etc 

----instace Type == CPU + Memory

- during the scalability , no data loss is occurs , coz data is stored in EBS volumes,downtime is there 

- if u have HA , no downtime

---------Brustable performnce instance 

- it is billable 

- whn there is lot of traffic coming to the ec2instance(t2.micro) it is not sufficient to handle 

- so aws will give cpu credits , ec2 will nter into the brustable mode and it give high performance for limited of time only

- "cpu credits" deped on the type of instances

- onlt t2 and t3 tyes supports "brustble prformance"


=====================Volumes

- nothing but hard disks

- root volm device name = /dev/sda1

- -------------------------------two types of volumes 

- 1 EBS Volumes 

- persistent storage/ permanent storage

- if u stop and start the ec2 instance DATA is not lost 

- EBS volume max sixex is = 16tb

- EBS is billable 

- if u reboot = data is not lost 

--------Types of EBS Volumes

- General Purpose (gp2,gp3) - SSD = general purpose

- Provisional IOPS (io1, io2) = Highly performance

- Throughtput(st1) - HDD = frequntly access data with cheaper price

- cold(sc1)- HDD = not frequently access data with cheaper price

- Magnetic(standard)-HDD = Previous generation

-- which volume is use when?

- for gneral purpose - gp2,gp3 

-  if u need HP = io1,io2

--- "gp2 is default EBS volume type"

--- IOPS = input and output per second , how mant inputs and outputs u are getting from harddisk

Note : io1,io2 , gp3 are iops configurable 

-- The more iops , the more Bill

-- GP2 has default IOPS = 1:3 =1gb:3IOPS -> it is not IOPS configurable

-- Root volume supports (gp2,gp3,io1,io2 and standard) except sc1 n st1

-- Additional volume supports all types of volumes


-- generally u cant attach volume to multiple instace at the same time 

- but io1 , io2 can be multi-attached at the same time in same AZ

-- up to 16 EC2 instance at a time can be attached 

2 Instace store volumes 

- not persistent / Temporry storage

- if u stop and start the e2 instance , data is lost 

- it is fre volumes

- Emphemeral storage -- another name of ISV 


NOTE : if u terminate the instance all ur root volumes get terminated by default coz, "delete on termination" is checked/enabled

-  if u want to retain the root volume do uncheck  "delete on termination" while launching te ec2 instance 

- if u terminate ec2 , by default "additional volume will not be deleted coz "DOT" is unchecked/disabled

- based on the size of instance, instance store volume will provided by aws 

-  if u reboot = data is not lost 

-  the reason behind the aws is that 


NOTE -- whenever if u store ur data in the EBS(central storage) , here once you do


- start and stop = Jump = data is not lost 

- Reboot = no jump = Data is not lost 

- terminate = Data lost 

Explanation : here in the EBS volume it is centraled stored so, when u do start and stop the ec2 istce it will jump from one host machine to another host machine ,due to centrally storage it will get the data volume without any loss


--- NOTE : when you store data in the "INSTANCE STORE"

- stop and start = jump = Data is LOSt 

Explanation : when there is instance store , a hard disk is attached by aws to the instance store ,so when there is jump happen here it will jump to another host machine but hard disk will coected to the host machine only , so data is getting lost 

-- Reboot = no jump = Data is not Lost

----------------to know the ec2 is launched properly or not 

- they are 2 types of status checks 

1 instance status check ( hardware check) 

2 system status check (software check) -- ip, network etc.....

-- status checks are done by aws 

NOTE: if u get 1/2 or 0/2 status check are passed , just STOP and START the ec2 instance

or 

 terminate it and launch fresh 


-- instance store volume give high performance 

-- EBS volumes are network drives with good but limited performnae




=====================================SNAPSHOTS===============

-- Backup of the volume is called "SNAPSHOTS"

-- Snapshots are incremental backups 

-- u can create volume from the snapshots 

-- snapshot is point in time copy of the volume 

-- snapshots does not contain any A.Z's

-- EBS volumes ---> snpshots --> ebs volums

-- u cant attach snapshot to the ec2,u have to create volume from snapshot

-- u cant login into the snapshot

-- snapshots are stored in S3(provided by AWS)

-- snapshots are regional

-- by default snapshots are PRIVATE , if requri u can make public

-- u can copy the snapshots across the region in the same account , and aws accounts also usng aws id (private )

-- ebs volms cant moved from one AZ to another AZ but u can take snapshots and use in anaother AZ

-- by default volumes, snapshots are not encrypted

-- Decryption is handled by AWS

-- NOT encryptd --> NOT Encrypted

-- encrypted --> encrypted

-- NOT Encrypted --> encrypted(Copy Option in ec2 u can make encryption)

-- all encryption keys are stored in KMS (key mangement service)

-- NOTE:  encryption snapshots cannot be shared to other accounts 

------------ instace store volms are crated from a template stored in S3

-- to create a snapshot u no need to stop the ec2

-- DATA LIFE CYCLE MANAGER : it is used to take snapshots automatically / sechedule

-- these volumes will get identified by using TAGS

-- RETEntion period = 7 days

------------------- EBS snapshot standard and Archive tier 

-- movethe snapshots to Archieve tier i.e 75% cheaper

-- it takes 24 - 72 hrs for restoring from archieve tier

---------------RECYCLE BIN

-- setup rules to retain deleted snpshots so u can recover them after accidental deletion, (retention period 1 day to 1 yr)

------------Fast snapshot Restore(FSR)

-- it is bilable

-- forceful initilization of snaphat to have no latency on the first use


===============================IMAGES===================

-- copy of the os is called IMAGE 

-- in AWS AMI = amazon machine images

-- template of os is called "AMI"

-- AMI --> copy of entire ec2 instance(includes volumes)

-- AMI contains OS or OS+apps

-- AMI's stored in aws S3

-- copy of the image includes all configuration that we did original istance

-- 1 AMI, can be used to launch multiple ec2 instane

-- it does not have any AZ

-- by default AMI are private 

-- AMI are regional 

-- AMI can be copied from one region to another region

-- AMI can be shared from one a to another aws account

--------------All public images are located in "market place"

-- GOLDEN AMI : images are created automatically 

-- creating image builder --> through : "ec2 image builder"

-- images are backed by either ebs volumes or ISV 



IMP NOTE : whenever if u create image(it has 2 volumes by default) , automatically 2 snapshots are generated 


-- no need to stop the ec2 to tke snapshot but it is recommended to take snapshot after stop the ec2 

==============================KEY-PAIR=========================



-- it is used to retrieve the password of the ec2 instance , we do not have any key-pair by default, we have to create it

-- the extention of the key-pair is ".pem"

-- aws has public key and we have .pem private key both called "key-pair"

-- u can attach key-pair to multiple instances but not many instances atthe same time

-- u can create multiple key pairs

-- ec2 instace can have only 1key-pair attached at any point

-- once the .pem file is attached,u cannot change the .pem file to the instace

-- every time u retrive the password of the same ec2 , u wil get same password 

-- keep ur .pem files in secure place



-- for windows:

- ip: provided by aws

- usename for windows : Administrator

- password : you will get it through key-pair

- to connect windows , u can use RDP protocol rdp/3389

- how to connect? 

ans : remote desktop connection client 



-- for Linux :

- username for windows : ec2-user

- password : you will get it through key-pair

-to connect linux , u can use RDP protocol ssh/22

- how to connect? 

ans : we use putty or mobaxtrem 

-- putty does not support .pem file , it supports PPK file 

-- puttyGen --> convert pem to PPK



-- do not share pem files to any one 


-to login into ec2 so many people u have instead of giving pem file in aws use "Directory servie"

-- join the all ec2 to the active directory , by this people can login without pasword 

-- keypairs only with admins 

-- thse types are called "domain user" login type 

==============================Cluster network instances=========

-- cluster = grp of servers ---> this group is called Placement group"

-- generally , when u launch a new ec2 , the ec2 service will place the instance such a way that all ur instance are spread out across diferent hardware

-- te speed b/w instaces is 20Gbps

-- we have 3 cluster network instance

1  Cluster Placement Group : 

- grouping the einstance in same rack same AZ, high performnace , low HA



2  Spread Placement Group : ec2 instance are spread across AZ's, high HA, critical Applications

- per 1 AZ = 7 EC2 instances u can launch 


3  Partition Placement roup : 

- Across AZ, Maz partition = 7 

- each partition has 100's of ec2 instances


note : placement group recommended to have same homogeneous instance type

- when you are lunching the ec2 , you can select which placement group u want to keep the instance 



==============================Security Groups========================

-- firewall = security group : which stops unauthorized access to the network

-- allow/deny

-- Sg has 2 types of rules 

1 inbound rules : allows traffic towards ec2

2 outbound rules : which allows the traffic outside ec2


-- by default , u dont have any inbound rules , it deny by default, u have give inbound rules

-- by deafult outbound rules are allowed 

-- it is not possible to deny protocol in SG , coz by default inbound rules are deny

-- in SG , we can only ALLOW protocols not DEny

-- every ec2 shouldnhave 1 sg

-- ec2 has a default SG

-- they are 3 types of source to access

1 custom : certain network

2 anywhere : through out the world 

3 MyIp : ur own computer 


-- u can attach multiple sg's

-- 1 SG --> attach to multile 


IMP : if u allow any protocol in inbound rule , u no need to allow that on outbound rule ----> these is called "STATEFUL"

Eg: SG

- if u allow any protocol in inbound rule , u must allow that on outbound rule  also ----> these is called "STATELESS"

-- Eg: NACL's


========================NACL(network Acces control list)=============

-- it is another layer of security to the ec2 

-- if u want tight the security go for NACL

-- like SG , NACL has inbound and outbound rules


-- NACL will hit first then SG

-- in VPC they aare multiple Subnets

-- 1 subnet is associated to 1 AZ

-- 1 subnet can't be in multiple AZ at the same time 

-- 1 AZ can have mltiple subnets

-- 1 NACL can have multiple subnets

-- NACL is subnet level and SG is ec2 level 

-- in NACL u can deny but in S.G u cant 

-------------------------------------------SG                   

-- inbound n outbound rules 

- default SG is there 

-- SG will hit after NACL 

-- by default , inbound rules are deny

-- u cant deny on SG

-- SG is Instance lvel

-- if u careate any new sg, inbound rules are deny , outbound rules are allowed 

-- SG are STATEFUL

-- if u allow any inbound rule, u no need to allow on ob rule 



-------------------------------------NACL

-- inbound n outbound rules 

- default NACL is there 

-- NACL will hit first 

-- by default , inbound rules are allowed

-- u can deny on NACL and allow also

-- NACL is subnet lvel

-- if u create any new NACL, inbound rules,outbound rules are DENY

-- SG are STATELESS

-- if u allow any inbound rule, u need to allow on ob rule also



===========================AUTO-SCALING===============

-- Scale out and scale in ec2 instance based on demand

-- scale out = adding 

-- scale in = remove


-- elb does health checks to the application 

-- Cloudwatch will monitor the ec2 istance


-- 3 types of scaling Options:

1  Manual Scaling : manually changed the min,max capacity


2 Schedule Scaling : u can schduled the scaling based on the period or a day 


3 Dynamic Scaling : based on metrics , cpu > 70% -- based on the load



--------------------------LAUNCH TEMPLATE/LAUNCH CONFIGURATION


-- how does ASG know that scalout ec2 will have appn of urs 

-- u can specify this launch temlte , this is create automatically all things in new ec2 

-- Custom AMI(app) or AWS ami , volumes, SG, 

-- ASG = ELB + EC2 instance + Launch template + SNS




=============================ELASTIC LOAD BALANCER==================

-- http n https 

-- which is used to spread traffic across all the instances

-- elb is maintined by AWS

-- elb is service for us , not a server , u can not login into the elb

-- elb can be access using DNS NAME or URL 

-- health check code is 200

-- elb has a ip addres but this is dynamic not static

-- aws always recommend to use the elb dns not the IP address

------------types of L.B's

1 classic load balancer :

- supports http , https, and TCP

- also know as previous generation


2  Appication Load Balancer 

- works on layer 7 

- latest generation

- default choose is ALB

- HTTP and HTTPS

- best for micro-services


-- ------------------it has routing features 


-- in ALB we r creating rules , we have target groups , so according to the traffic the request will send to different target groups 


1  host based Routing : https://subbu.com

2  Path Based Routing  :   https://subbu.com/admin

3  String Parameter Routing :  https://subbu.com/course=aws?


note : this is not possible in the CLB , so it is not used now 



3 Network load balancer 

- u willget fixed ip addres here 

- works on layer 4

- latest generation

- TCP AND UDP, TLS 

- Extremet high performance

- network level 

-  "it provide 1 static ip per AZ"


4 Gateway Load Balancer 

- works on layer 3

- deply , manage and scale a fleet of 3rd party network virtual applications in AWS

- f u want to set up any firewall,prevention system etc

- it uses GENEVE protocol on 6081



==================================TYPES OF IP's=================

1 public IP  : it is not manditory ,this is optional , it is dynamic , it will change when u do startand stop the ec2


2 Private IP : by deafult u will get private ip ,when u launch ec2 ,it is manditory 



3 Elastic IP : the ip wont change when u start n stop the ec2 , it is static ip address

- it is same as public 

- 5 EIP's are free 

- if u not associated with any ec2 instance, aws will do chrge for ideal eip's




-- with in the AWS 2 ec2 wants to talk , the privatae ip is used 

-- instance meta-data = data about ec2 instance 

- from console , u get meta-data from Details section



-- from CLI , follow this below URL 

- http://169.254.169.254/latest/meta-data/



-- USer-Data = Bootstrap Scripting 

- the script which u have provided will run at the boot time of the ec2 instances 


-- user data will run only for the first time of launching the ec2 

linux = shell script

windows = powershell


-------------------------------7 steps to create ec2 

1 select AMI (linux , windows .......) 

2  instance type 

3 instance configuration = how may instance , public ip , user-dta 

4 select storage = ebs ,root volume additional volm

5 Select SG 

6 add Tag 

7 create PEM file and review 



==================================Global Accelerator(GA)====================================


-- managed by AWS 

-- it use aws network , it uses aws edge locations 

-- it has low latency and high performance 

-- if u want to setup with static ip we choose "GA"

-- it provide 2 static ip's

-- it is billable 
 

-- unicast ip : one server holds one ip address 

-- Anycast ip : all server holds same ip address and client is routed to the nearest one 




-- eth0 --> private IP

-- eth1 --> public ip



===============================EC2 console============

-- ec2 global view :u can view resoure globally 

-- events : any maintainenece is there , it will show u in the events sections 

-- launch tmplate : i ASG we use L.T ,it contain configurations 


-- capacity reservations : u can reserve the cpacity ,when u need requriment 

- for only reserving u no need to pay until u will use this 

  ==========================EFA === elastic fabric Adapter 

- it is a network device that u can attach to ur instance to reduce latency and increase throughput for distributed high performnace computing(HPC) and ML 


-- advanced 


-- hibernate mode : to put ec2  in sleep mode 

when to use ? 

- when u have big machines , so much data inside in it , that time u do hibernate it will not charged for hibernate 


===================ec2 Hibernation==============================

--  Amazon has introduced a new EC2 Hibernation feature which when enabled the Hibernation it saves the contents from the instance's memory that is RAM to your Amazon EBS root volume.

-- in genreal whenevr we do 

Stop : if we stop the ec2 instance , the data on ebs volume is persisted

Start : if we start the ec2 instance , the data on ebs volume is restored

Terminate : it delets the ebs volume also 

Hibernation : saves the content from the instance memory (RAM)to ur AMazon root volume


-- the main theme of Hibernation is that when u do not want restart the whole processer 

Eg : in our daily life u can do some work in ur personal computer and do shut-down when it is over , again when u do start ur system it will take some time to start and in the backend it is booting up the system , so to avoid this we can use Hibernation

-- AWS persists the instance's Amazon EBS root volume and any attached Amazon EBS data volumes.

-- Here is the pre-requisites for using the hibernation feature:

1 EC2 hibernate instance root volumes must have encryption enabled to ensure the security of the content stored at EBS.

2 Hibernation is not supported with all of Instance types but it is supported with following instance types: C3, C4, C5, M3, M4, M5, R3, R4, R5, and T2.

3 Instance RAM size should be less than 150GB

4 AMIs supported: Amazon Linux 2 AMI, Amazon Linux AMI, Ubuntu, and Windows

5 EC2 instance hibernate must be backed by EBS volumes, instance stores aren’t supported.

6 Available for the On-demand , Reserved instance and Spot instance

-- 


-- Limitations on the EC2 instances enabled for hibernation

1 You cannot increase/decrease the size of hibernated instance.

2 Cannot enable snapshot or AMIs from instance in hibernation state.

3 You can’t enable hibernation after launch.

4 If an EC2 instance is enabled for auto-scaling group or used by Amazon ECS, then you cannot hibernate that instance.

5 An EC2 instance cannot be kept hibernated for a period of more than 60 days. To keep the instance for longer than 60 days, you must start the hibernated instance, stop the instance, and start it.

6 To hibernate an instance that was launched using your own AMI, you must first configure your AMI to support hibernation.


-------------------LAB-----------------------

-- create one instance A with hibernation , do encrypt ur EBS volume while creating and use aws key for encryption

-- create another instance B  , without hibernation 

-- do connect A and type uptime , it will shows the time from how much u are active state 

-- do same for instance B 

-- do stop the instances and wait for 1-2 min 

-- do start the instance again 

-- connect the instance A and do uptime it will give the time from the time when u started the instance A , coz it is hibernated.

--  do same with the instance B , when u do for the instance B , it will show the time from 0 , coz it is not hibernated 

Use Cases : 

-- Long-running processing 

-- saving the RAM state
-- Services that take time to initiliaze 





-- termination protection 

- enable : u cant terminate 

- disable : u can able to terminate
 

-- same stop protection also -- u cant stop the server 


-- EBS optimized instance:  whenwver ur data in volumes that will store in central EBS 

- like the same way some ec2 will have optimized with the ebs , and it will give high performance


---------Nitro Enclave 

- A Nitro Enclave is a "trusted execution environment (TEE)" in which you can securely process sensitive data. It extends the security and isolation characteristics of the AWS Nitro System and allows you to create isolated compute environments within Amazon EC2 instances. If no value is specified the value of the source template will still be used. If the template value is not specified then the default API value will be used.

- Nitro Enclaves are not compatible with instance types that have less than 2 vCPUs.

- it is chargable

----------------

-- if public ip address changes for ec2 , public dns also will change 


----------------------imp points:


NOTE : aws use "xen" virtualization

-- instance screen shot : to know it is reboot is done or not 

-- while creating the image(ami) u cant change root volume properties except size of ebs 

-- while creating the image we can also add additional volume 

-- u can also select instance store volume as a additional volume 

-- if u ec2 root volume is not encrypted while creating the image the root volume is also not encrypted

-- while creating the image additional volumes can be encrypted

-- during the image creaton process ec2 create snapshot of each of the above volumes 

-- once u check in AMI , the image is created 

-- u can copy the image and create image across regions 

-- copy AMI provide 2 things ( copy imge to other region and image can be encrypted)

-- deregister = delete

-- by deafult images are private , u can also transfer to anothr account using account id 

-- deprecation = out of date


--- recycle bin 

-- u need to create retention rule in recycle bin 

-- R.B can be implemented on snapshots and AMI 

-- not all snapshots and every ami wil not go into recycle bin

-- u can specify whiich one to retention by using tags 

-- if u encrypt snapshot u cant share to others 



---------- u can create image from the snapshot also which has os





======================volumes practicals==========================

--  create 2 nstaces and automatically 2 root volmes will be created 

-- u can deattch root volume only after stop instance 

-- force deattch : this is additional volumes to forcefully deattach 

-- u can increase size of volume without stop the ec2 

-- select volume and go for modify volume and give ur cpaacity as per requriment 

----create lifecycle policy

-- automatically create snapshots and images 

-- Enable cross-Region copy to copy snapshots created by this schedule to up to three additional Regions. if u want only

-- Enable cross-account sharing to share the snapshots created by this schedule with other AWS accounts.


NOTE : if u lost PEM files , how to recover password ? 

- crate new pem and instance 

- new instance has root volume 

- now deattach the old root volume and attach to th new instace as  a additional volume now new instance has 2 root volumes 

- go to old root volume and modify one file in d-drive and deattach from new instance and attach to old instance and u can login 


---------------EBS practicals with volumes


1 Ceate instance -A

2 create one volume and attach to the instace-A

note: it should be in the same A.Z

3 now connect instace-A and try to mount.

4 check if it attached any extra volumes or not by using command lsblk

5 if no volume is attached then , it will looks like

NAME      MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS
xvda      202:0    0   8G  0 disk 
├─xvda1   202:1    0   8G  0 part /
├─xvda127 259:0    0   1M  0 part 
└─xvda128 259:1    0  10M  0 part /boot/efi


6 if any volume is attached , then it looks like

NAME      MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS
xvda      202:0    0   8G  0 disk 
├─xvda1   202:1    0   8G  0 part /
├─xvda127 259:0    0   1M  0 part 
└─xvda128 259:1    0  10M  0 part /boot/efi
xvdf      202:80   0   1G  0 disk --------------attached volume


7 Now switchto root user ( only root user can able to mount) ---- sudo su

8 now create file direcory by using command
 
   sudo mkfs -t ext4 /dev/xvdf

9 now create a directory to mount the volume  by using command 

 sudo mkdir <<your foldername>>

10 now mount your EBS Volume by suing command

sudo mount /dev/xvdf <<foldername>>

11 now check it is mounted successfully or not by using command  

    lsblk

12 if it is mounted, then it looks like

NAME      MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS
xvda      202:0    0   8G  0 disk 
├─xvda1   202:1    0   8G  0 part /
├─xvda127 259:0    0   1M  0 part 
└─xvda128 259:1    0  10M  0 part /boot/efi
xvdf      202:80   0   1G  0 disk /home/ec2-user/MyRules

here MyRules = FolderName


13 now switch to mount point directory

cd MyRules

14 create some files in it and save

eg: hello.text and write some txt inside it and save

check to see all the files by usng command ls -ll, it will show you all files that you have created

15 Now detach the volume and create snapshot from it and go to napshot and create one volume on different A.Z from 1st instance and attach this new created volume to the 2nd instanc ( volume and instance should be in same A.Z in EBS)

16 now connect the instace-2 and enter command to check any file system is there or not by using command

sudo file -s /dev/xvdf  

17 once you enter above command you will get like this , if any file system is presnet

/dev/xvdf: Linux rev 1.0 ext4 filesystem data, UUID=ee2699ae-ba2c-4805-941a-f390c4344a37 (needs journal recovery) (extents) (64bit) (large files) (huge files)

18 folow the same steps from  9-13 , and afer enter ls -ll command to see all files

19 once you did successfully mounted then you will able to see the files that you have creted in the 1st instance time.



===========================LOAD BALANCER=============================

-- LB is regional 

-- in linux all start with /Root

-- u have so many likw /var /etc /usr /home /tmp /bin 

-- when u login in linux ur home diectory is created 

-- /var = all file logs will be there 

-- /tmp = temporary 

-- /bin = all the softwares available 

--/etc = all ur system files will be there 

-- /usr = all things related to user 



 haredware --> kernal --> shell--> cmd = linux structure 

-- all the shells under /bin

-- bash shell is new one , located in /bin/bash


-- when u enter any cmd in linux like touch , ls , it wil go to shell --> kernal --> hardware then get reply to cmd 

-- u cant execute any cmnds without shell script

eg for shell script (user data)

#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html



exlanation 

#!/bin/bash  -- shebang 
yum update -y 
yum install -y httpd --  --- installing httpd service

systemctl start httpd --start 

systemctl enable httpd -- auto start ttpd service

echo "<h1>Hello World from $(hostname -f)</h1>" > 

/var/www/html/index.html --default directory httpd service will create 

> = redirect , it will create file 

-- whatveer in echo with > symbol will ceate index.html file automatically

-- whatever the data in echo all the datais stored in index.html file 

-- put this script in userdata


-- for L.B listeners should be http/80 and https/443

-- the facing should be internet facing = public a

-- internal facing is private and access within the vpc only

-- ACM == amazon certificate manager , where u can buy certificates


----------------------

-- create 2 instance with the user data 

-- one instance in 1a AZ and another instance is 1b AZ

-- now go to SG allow http 

-- check th user data is working properly or not by copying public ip of ec2 both and pste in brower u will get o/p 

-- once u check and it is working prprly , u can create LB now 

-- once u create LB , ucan copy  dns and paste in the browser , see hoe taffic is distributed across the multiple instance 

-- if u have heavy applications , make ideal time out more in attributes section 

- all the logs stored in s3 buckets 

-- WAF : Protects your web applications by enabling AWS WAF directly on your load balance

- it is used to secure the web applications ( DDos attacks like hacking ,sql injections or any hacking methods ) 


---- ----------------------------------in Target Groups

--  in attributes 

1  Deregistration delay (draining interval) : it means for eg we have 2 instance , if one goes down 2nd will take care , if 2nd also goes down there is downtime so if u donot want downtime we implement ASG 

- during the scaling time (scale in) removing the server1 , some client may be connecte to server 1 so connection wil be there , so ASG wont delete the srver1 immediately 

- so the concept deregister delay means , it wait for 300 seconds , all in-flight(requests which are in progres) request wil get completed after that only it will deregistered from the target group 


2  Slow start duration  : when scale out happen , LB has to sent trafffic to new server 


3 Load balancing algorithm : round robin method it uses (1,2,3,1,2,3)

- u also have Least outstanding request : this means u have server 1 and server 2 based n the request like if serve 1 getting 5 request and server2 is 20 requests , the nxt request is going to server 1 coz it is getting low requests , it is based on the least getting requests 


4 Stickiness : when ever u want to stick with to the specific server 

- if u enable stickness , ur request sent to only one server for the particular period of time 

-------------------------------------- do for /path based routing 

-- create 2 more servers 

-- with admin in the user-data 

-- create 2 ec2 with userdata

#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/admin

-- once u created jst copy public ip and paste in browser  "192.87.98.2/admin"

-- now add target groups in LB 

-- once u create TG and attach to LB and create rule 


-- go to ELB --> listeners and rules --> edit rules --> add condition(path) /admin --> 

-- u have one option like return fixed response : 

select path rule --> select return fixd response --> enter response code and write ur text 


	
Forward to target group

Admin-tg : 1 (100%)
Group-level stickiness: Off


-- once u created rules here , check in browser by giving /admin 

-- this is called path based routing 

-- limits for ALB 

-  total 100 ules per ALB 

-  5 Conditions values per rule 

-  5 wildcards per rule

-  5 weighted target group pe rule 


-- for https we have certificates 

-- go to ACM and c.o request certificate 

-- select request a public certificate -->nxt

-- provide Domain name : eg : *.subbu.com ------ here * means any sub-domain

-- u cna purchase the domain first and then create record

-- then in target groups select https in target groups and add liteners 



===============================AUTO-SCALING GROUPS========================================


-- create a LB empty TG

-- u have to create Launch template (user-data), u can edit LT once u created , u havve create new LT if u want new version of ur appn 

-- c.o on ASG and Give name as you want and select Launch Template(IMP:Make sure AMI Version is with Kernal 5.10.....) ,select the version of template you want

-- using with LT, create ASG 

-- ASG use LT to launch ec2 instance 

-- if u want ur big applicatio like tomact on ec2 instance(ASG), cretae ec2 first ,deploy the appn and create AMI 

-- use this custom AMI in LT instead on Linux and create LT 

-- u provide min , max and desired capacity while creating ASG 

-- ASG will launch ec2 in TG automatically 

-- Always Turn on ALB Health checks 'coz, 

Elastic Load Balancing monitors whether instances are available to handle requests. When it reports an unhealthy instance, EC2 Auto Scaling can replace it on its next periodic check.

-- here we need to use sns ( simle notification servie) 

-- in SNS , we have TOPIC and SUBSCRIPTION 

-- in sns u need to crate TOPIC , once u created u need to subscribe

-- u can add this topic in ASG creation

-- create LB with simple TG no need to add any instances as targets ,this job will do by ASG for us during scale out and scale in process

-- now launch template with user-data

#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html

-- create auto scaling group 

-- give LT and subnets as u preffered 

-- in notification topic u jst create topic to get notifications 

-- crate ASG 

-- what this ASG will do is that , it wil take LT and it will launch ec2 automatically according to ur desired capacity , so LT has user data so then our ec2 have index.html appn,all the ec2 instances will registered in the target groups automatically  and these TG are linked to Load balancer , so if u can access LB u will get appn 

-- all the things u have do in the launch template only , based on these LT ec2 will created 

-- go n check in target group 

-- in targets all ec2 are gettig registered 

-- There are 3 types f scaling 

1  manual scaling : u can change manually capapcity 

2  schedule Scaling : do schedule when u want to scale out happens when u feel the traffic will more o the certain days or time 

3  dynamic scalng : automatically happens 


-- u can check in activity section how scale out and scle in happens 

-- once it reach to max capaicty it wont go more than that as th given capacity , u can edit once if u want more capacity to scale out 




---- there are 3 types of Scaling policies in ASG 

1 Target Tracking POlicy :

When you create a target tracking scaling policy, Amazon EC2 Auto Scaling automatically increases and decreases capacity in response to varying usage levels. For example, a target tracking scaling policy might have a target CPU value of 50 percent. Amazon EC2 Auto Scaling then launches and terminates EC2 instances as required to keep the aggregated CPU usage across all instances in your group at 50 percent.



2 Dynamic scaling policies : 


Amazon EC2 Auto Scaling supports the following types of dynamic scaling policies:


A  Target tracking scaling—Increase and decrease the current capacity of the group based on        a Amazon CloudWatch metric and a target value. It works similar to the way that your thermostat maintains the temperature of your home—you select a temperature and the thermostat does the rest.

B  Step scaling—Increase and decrease the current capacity of the group based on a set of scaling adjustments, known as step adjustments, that vary based on the size of the alarm breach.

C  Simple scaling—Increase and decrease the current capacity of the group based on a single scaling adjustment, with a cooldown period between each scaling activity.
 

--- How dynamic scaling policies work

A dynamic scaling policy instructs Amazon EC2 Auto Scaling to track a specific CloudWatch metric, and it defines what action to take when the associated CloudWatch alarm is in ALARM. The metrics that are used to invoke the alarm state are an aggregation of metrics coming from all of the instances in the Auto Scaling group. (For example, let's say you have an Auto Scaling group with two instances where one instance is at 60 percent CPU and the other is at 40 percent CPU. On average, they are at 50 percent CPU.) When the policy is in effect, Amazon EC2 Auto Scaling adjusts the group's desired capacity up or down when the threshold of an alarm is breached.

When a dynamic scaling policy is invoked, if the capacity calculation produces a number outside of the minimum and maximum size range of the group, Amazon EC2 Auto Scaling ensures that the new capacity never goes outside of the minimum and maximum size limits. Capacity is measured in one of two ways: using the same units that you chose when you set the desired capacity in terms of instances, or using capacity units (if instance weights are applied).

* Example 1: An Auto Scaling group has a maximum capacity of 3, a current capacity of 2, and a dynamic scaling policy that adds 3 instances. When invoking this policy, Amazon EC2 Auto Scaling adds only 1 instance to the group to prevent the group from exceeding its maximum size.

* Example 2: An Auto Scaling group has a minimum capacity of 2, a current capacity of 3, and a dynamic scaling policy that removes 2 instances. When invoking this policy, Amazon EC2 Auto Scaling removes only 1 instance from the group to prevent the group from becoming less than its minimum size.


3  Predictive scaling policies : 


Predictive scaling forecasts load based on your Auto Scaling group's history. It scales out the group in advance of forecasted load, so that new instances are ready to serve when the load arrives.


Predictive scaling works with CPU utilization, network in/out traffic, the request count to an Application Load Balancer target group, and custom metrics.

You can use predictive scaling to improve availability for applications whose workloads have predictable daily or weekly cycles.

As a best practice, consider using both dynamic scaling and predictive scaling. Predictive scaling uses forecasts to make decisions about when to add capacity according to a metric's historical trends while dynamic scaling makes adjustments in response to real-time changes in a metric's value.



-- instance Refresh : if u want new version of ur apn , then cretae new LT coz u cant edit old LT so create New LT and attach to ASG 

-- this is how u can do updates in real time in ASG 

Start an instance refresh to perform rolling updates on the Auto Scaling group's instances. Only one instance refresh can be active at a time.


-- when ec2 terminating it is showing like "draining" once it finishes all connctions , then only ot will get removed from the TG (it will complete in-flight request and get removed)

---------- scaleout practicals 

-- (IMP:Make sure AMI Version is with Kernal 5.10.....) ,select the version of template you want

-- conect one instace to SSH and install manual load to the EC2 instance by install stress commands manually

--  enter the command

 sudo amazon-linux-extras install epel -y 

-- once avove command is successfully installed , then enter this command

  sudo yum install stress -y

-- once you installed, then try to push the load manually by entering the command

  stress -c 5

--  wait for sometime(4-5 min) to updating the data , then check in ASG and see how the changs are happening

-- u can see the scale out is happening and meet the max capacity 


============================CLOUD WATCH=============================

-- CloudWatch is all about Alarms,Events and Logs

--it is regional only 

-- CW is used to monitor performance of all as resource 

-- to monitor the resource , CW need Host level metrics also known as default metrics 

1 CPU 

2 Network 

3 Disk --- volume 

4 Status Check 

-- memory not comes under HLM 

-- memory is custom metrics 

-- 2 types of moitoring 

1 Basic and 2 Detailed Moitoiring 

-- Basic is free nad it will tke every 5 min data

-- deatiled monitoiring : every 1 min data points , billable


-- u create alaram in CW 

-- alarms can do actions like (terminate , reboot ,stop , recover ) 

-- Alarm has 3 States : 

1 In Alarm : > 90 

2 OK : < 90

3 Insufficient : ec2 stopped due to some reasons



-- we also hve concept called " Composite Alarms"

- CW alarms are single metric

- Composite Alarms are monitoring the states of multiple alarms 

 eg: AND or OR conditions 


============================EVENTS/Event Bridge=========================

-- where all th events are stored 

-- u can use these events for many use cases 

eg: if u want to get notification once ec2 is stoped 

ec2 --> stopped --> event -->route to the target (SNS(topic))


-- u can also do scheduled/Cron Job 

eg: for stopped and started 

 for this we have to create 2 lambda functions one is for stop and another one is for start 

--  NameSpace : group of metrics / collections of related metrics

-- if u want to get all ec2 logs at one place , u have to add CW agent in each ec2

-- all the logs visible in CW logs 

-- by default CW agent not able to push logs to CW logs 

-- by using Roles u can do push the logs to CW LOgs 

---- in CW we have Cannary/cannaries : used to applications monitoring websites endpoints


=============Practiclas Alarms Events and lambda ========================


-- ----CloudWatch


-- crate 2 instances 

-- now these ec2 not in use so ,if u want to stop the ec2 which is not in use ,

-- create alarm first --> c.o + in instances alaram status

-- create alarm as per ur requriments and see how it is stopped the ec2 instance 

-- as of now we have created alarm in ec2 console 

-- we can also create alaram in CloudWatch 

-- go to CloudWatch 

-- c.o create alarm --> select metric --> ec2 --> Per-Instance Metrics-->select ec2 u want copy id --> slect hostlevel metric --> it wil create one namespace like aws/ec2 

-- check how alarm works 

-- IMP : Alarm actions (stop,terminated,reboot) enable and disable at any time 

---------------Events/eventBridge

-- in EB , we can create Rules 

-- u can receive notificatio if someone stop ec2 

-- create rule in event bridge as per ur requrimnts

--------------- automation lambda  

-- if u want to stop ec2 at exact time and start exact time we use lambda for the automation 


1 create policy to stop ec2 

2 create role and attach policy 

3 attach this role to lambda -- trusted entity 

4 event schedule 

-- to folow these steps u can stop ec2 

-- lambda with vpc = can acces aws sevices like rds,s3 etc 

-- lambda without vpc = public 

-- now go to iam inline create policy 

{  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:*:*:*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:Start*",
        "ec2:Stop*"
      ],
      "Resource": "*"
    }
  ]
}


------ now create a role 

-- select lambda and attach policy that u have created 

----------- now create Lambda function 

- lambda is region only 

- it is serverless

- create functio and attach role to the lambda and write code for stop function in python 

import boto3
region = 'us-west-1'
instances = ['i-12345cb6de4f78g9h', 'i-08ce9b2d7eccf6d26']
ec2 = boto3.client('ec2', region_name=region)

def lambda_handler(event, context):
    ec2.stop_instances(InstanceIds=instances)
    print('stopped your instances: ' + str(instances))


-- --------create rule in event bridge 

-- create rule with cron job/schedule 

-- create cron job and if u wnt to know it is working or not jst refresh the lambda page u wil get triggers 

-- if u test ur code , if it will get executed successfully , the ec2 instacnes will get into actions(stop,terminated) automtically 

----- Lambda Limits 

- memory allocation = 128MB-10GB

- max Excution time = 900 seconds(15 min) 

- environmenta variable = 4kb in size 

- disk capacity in the function containers (in /tmp) like libraries = 512MB-10GB

- Concurrency execution = 1000(can be increased) , one lambda fun cab be executed 1k times 

-------- deployement 

- lambda function deployement size (compressed .zip) = 50MB

- size of uncompressed deployement = 250MB

- Memory allocation is very imp in lambda functions 

- lambda will be only billed onexeccution times 


===========================Cloudwatch Theory=============================

-- it is a service that collects and manages operational data 

-- operationl data and ny dtaa that u collected by an environment either detailing how it performs , how it normally runs or any logging data it generates 

---- we have 3 teminologies 

1  Metics : collects data of AWS Products , Apps and even on-premises servers

2  Cloudwatch Logs : logs of AWS products , Aps, on-premises

3  CloudWatch Events : AWS Services and Schedules 

Eg : it generates C.W events when EC2 stops , start or anything 


-----------some  terminologies to nderstand for the Cloudwatch 

1  NameSpaces : Containers for monitoring data , it is a way to keep things seperate 

-- NameSpace has got a name , it can be anythng as long as it stays within the rule set 

-- All aws data goes to aws namesoace ---> AWS/Service

-- Namespace contains related metric 

2  Metric : it is a collection of related Data Points , in the time ordered structures 

Eg: cpu usage network IN/OUT 


3  Data Point : let us say we have metric called CPU Utilization , everytime any server measures its utilixation and send it into cloudwatch that goes into the CPU utilization metrics and each one of those measures so everytime the server reports the cpu that measure is called "Data Point" 


--it has 2 components 

1 timestamp 

2 value 


Note : CPU utilization metic could contain data from many servers ,so how do we seperate data for this? so use " Dimesions"


4  Dimensions : these are Name Value Pairs that seperate data point for different thngs or perspective withn the same metric 


-- while sending data points to cloud watch ,AWS also send in , these two 

A) Name = InstanceID , Value =I-xxxx

B) name = InstanceType , value =t2.micro.......


5  ALARM : CW also allows to take actions based on metrics which is done using Alarms 

-- two states 

A) OK --> Everything is working fine 

B) ALARM --> Something bad has happened 






==============================================CLoudWatch LOGS=============

--- u ave to install CW Agent 

-- by deafult CW agent do not have permission to ush logs to cloudwatch logs 

-- so we should create ROLE here , and give CWFULL access policy and this role is attach to the ec2 

1  create IAM ole and give CW permissions 

2  attach The role to the ec2 instance ,make sure ur kernal version is 5.10....

3  login to te ec2 and install CW agent 

4  configure the files 

5  start the CW agent service 

6  see the logs in CW loogs 

-- crtae IAM role 

-- give CLOUDWATCHFILLACES permissions and attach role to the ec2 

-- now loginto the ec2 and enter cmnds 

1 make sure ur kernal version is 5.10....


- login as root user like sudo -s
2 in real time u always do Yum Update 

3 yum install -y awslogs  --- to intall CW logs

4 once  u install cw packages , u have to create 2 files 

- go to cd /etc/awslogs/

- press enter 

- once u do ls , two files will be crated 

-  1  awscli.conf and  2 awslogs.conf

- do sudo cat awscli.conf

- change region as per ur requriemnt

- do cat another file awslogs.conf

- file = var/logs/messages (this is appn log path) 

- log_stream_name = from ehre logs coming 

5 now u have to start awslogs , once u start awslogs automatically the log group generated once u start 

6 start the cloudwaatch Agent service

- systemctl start awslogsd 

- press nter 

7 once u pres nter the backend process will run and all the system logs will push to the CW logs 

8 go n check in log groups 

- /var/log/messages  log group created 

- we have given log_stream as Instance _id open n check the log group 

- once u open instnce id u can check all the system logs 

9  u can also push appn logs to CW agent 


- insted of var/log/messages u put log path   



--- u can do monitor for appliction using canaries 

- application monitoring --> synthesis canaries --> create canary for ur application and check the appn is working or not 

- select heartbeat canary for simple eg

=========================Lightsail================

- everything is availabe here 

- open lightsail

- here if we create instnce it is called "lightsail instances"

- create instance sleect wordpress

- here backup of lightsail instance is called "snapshot" 

- it only give 32GB only 

- once u create lightsail instance copy ip and paste in browser 



Expore topic : till now we only see Host level metrics , now find how to get metrics for memory 

- By default, we cannot monitor Memory metrics on EC2 Instances.

- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-scripts-intro.html

- use this link for the documentation 

or u can do in nother process

- create ec2 instance 

- crate one role and attach policies 

-  attach policies cloudwatchfullacces and AmazonSSMFullAccess

-  why SSM? 

ANs: i have to store particular valur(json document which is used to fetch the memory utilixation from aws ec2 and send it to AWS cloudwatch ) in ssm

-  create a parameter in the system manager with the name (u can givve any name) 

eg :  /alarm/AWS-CWAgentLinConfig 

- go system manager -->paramter store --> give name --> in the value place copy json script 

{
	"metrics": {
		"append_dimensions": {
			"InstanceId": "${aws:InstanceId}"
		},
		"metrics_collected": {
			"mem": {
				"measurement": [
					"mem_used_percent"
				],
				"metrics_collection_interval": 60
			}
		}
	}
}


-- ceate ec2 and attach role to ec2 and create with userdata with cloudwatch agent to install

userdata: 

#!/bin/bash
wget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip
unzip AmazonCloudWatchAgent.zip
sudo ./install.sh
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:/alarm/AWS-CWAgentLinConfig -s


-- check whether clouwtch agent is instaled or not by using 

-- sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status

-- if it is running it is success

-- now go to CLoudwatch and --> all metrics --> CW Agent --> give instacne id and chck logs of memory


=======================================Elastic Beanstalk==================

-- it is use for easy and wuixk deloyment of appn in aws

-- it is PAAS

-- backbone of beanstalk is EC2

-- beanstalk is free but what veer the resource launchs in backend that will be charged 

-- Beanstalk has deployment Modes (presets)

1 Single Ec2 instance : it will launch only one instcnce but it is not good for production environment 

2 High Availability : here u jst give configuration , automaticaly beanstak will prepare for u , it is preffered for the production environment 


Note :there is one more Custoom preset : when u have ur own configuration is called "custom reset"

----------------------Architecture of Elastic Beanstalk


--firt thing u have to create is appn 

-- inside appn u have environment 

-- inside the environment u have ur ec2 instance 

-- once u create env-url wil generated 

-- u can also have multiple environments 

-- if one ec2 is there in environment then it is called single deployment PRESET


-- if u go for High availablity , then u have ELB, ASG top of it and u can have multple instances 

-- u can acccess using env-URL 

-----------------for eg u have latest verion of ur appn 


-- beanstalk has deloyment Method/policy 

1  it has All at Ocne = all instaces go and update at once 

- here there is downtime , no xtra cost 

2 Rolling with additional batches : u wil give batch wise 

- No downtime here , it has  extra cost  

3 Immutable : if u have ASG running , it has 3 ec2 

- if  u g for immutable and ask for deploy new version then it wil create temporary ASG n then new version deployed in Temp ASG once it deployed succesfully then only it will move to original ASG 


4 rcently AWS cam with new one called traffic Spliting 



traffic Spliting  : split traffic b/w ec2 one by one 


5 Blue Green deployment : 

- eg if ur changing the complte platform , but deal is customer never face the downtime 

- All the previoud methods touch customer environment whic is not good 


- here u will create new environment and deloy new version of appn 

- in Aws it has feature called SWAP URL , once u SWAP ,the customer redirected to new platform 

- the existing one is called BLUE ENVIRONMENT and New one is called GREEN Environment 

- it has extra cost 


--------------Practical Elastic bean stalk(blue-green deployment) ----------


-- it is regional 

-- it is all about configuration 

-- There’s no additional charge for Elastic Beanstalk. You pay for Amazon Web Services resources that we create to store and run your web application, like Amazon S3 buckets and Amazon EC2 instances.

-- c. o create appn 

-- u have 2 environments here 


1 Web server environment: Run a website, web application, or web API that serves HTTP requests.

2 Worker environment : Run a worker application that processes long-running workloads on demand or performs tasks on a schedule.


-- choose web server 

-- Environment information 

- name :ELASTICBEANSTALK-tomcat-env

- copy n paste in url(domain) section below 

- crate one role which has ec2full access


-- step 3 : do not enable ublic ip for ec2 instances  


-- health report - basic 

--once u created ElasticBS succesfully it will crate one ec2 instance, SG, EIP and also instance added to Asg automatically 

-- u can all the events in event section below 

-- once u open domain u will get ur first application in tomacat platform 

-- if error chckc SG configuration or copy public ip and paste in browser 


------ now create new environment , ow we are changing platform python in appn 


-- same steps jst change platform  

-- once u launc new environment with python , click on domain link now u eill get in green colour for the python 


-- now u want ur customers redirected to the new version(python) 

-- do SWAP URL 

-- go to application level --> go inside tomcat server --> swap environment domain --> slect swap platform 

-- once u swap url all the customers will get in the green colour , here there is no downtime , url is not changed 



-- save ur configuration for the furute purpose if nay one canges ur configuraations 

-- done with Ebeanstalk



=============SIMple storage service (S3)================================

-- it is object based storage service 

-- in s3 u can store all kind of files 

-- S3 cost is depend on size of the object and data transfer 

-- u can only downloaded, upload and access files from s3

-- u cant instal os,db in s3

-- the files can not executed 

-- s3 is unlimited storage 

-- s3 is cheaper then ec2

-- s3 is serverless

-- s3 is global 

-- bucket = container objects 

-- object = file 

-- key = name of the Object 

-- buckets are regional 

-- bucket name are universal or UNique 

-- all ur buckets shown in same consl only even if u create buckets acrss region 

-- no nested buckets not possible 

-- u can create folders in bucket

-- by default , all buckets are private , if require u cna make public 

-- MAX 100 buckets per account, u can increase by conctact AWS ====imp

-- every file has URL to accesss 

eg : htps://bucket1.s3.ap-south-1.amazonaws.com/photos/puppy.jpg

-- sub-folder are called Prefix 

-- .jpg called suffix 

-- puppy.jpg called object 

-- photos/puppy.jpg === KEY 

-- it is WORM Model == write once read many 

-- ucan not get back once u delete but u can by using versioning 

-- 2 types of buckets

1 General purpose buckets are the original S3 bucket type and are recommended for most use cases and access patterns. General purpose buckets also allow objects that are stored across all storage classes, except S3 Express One Zone.


2  Directory buckets use the S3 Express One Zone storage class, which is recommended if your application is performance sensitive and benefits from single-digit millisecond PUT and GET latencies.

-----------------------------------S3 Versioning 

-- when u have critical data it is help to bacjup

-- Versionong is like a backup tool 

-- by deafult versioning is not enabled 

-- this cna be also do while creating bucket 

-- enable on bucket level and applied to objects 

-- Version ID is always Unique 

-- if u delete original ,it will have marker (delete marker) applied to  new version 

-- to restore object , delete the mrker and ur object is restored automatically (latest version) 

-- but if u want to previous version to be restored , u have to download it nd upload it again 

-- it is possible to download version files 


-- it is not posible to download delete marker , u can only delete it 

-- delete marker is applied to only latetst version, not for old/previous versions 

-- once u can enable versioning u can't disable it but u can do suspend it 

-- u can not restore the files when u suspend the versioning 

-- when the version is suspended it wont effect on previus objects but coming objects get effected 


----IMP POINTS 

-- min object size = 0 bytes , MAx objct size = 5TB

-- y can have unlimited number of objects having 5TB each in a single bucket 

-- if u have file 5TB u can not upload it in one shot , so in aws recommened MPU(multi-part-upload) 

-- this will done through the CLI not from the console 

-- aws recommended , if u have object > 100MB , go for MPU 




==============================Storage classes============================

-- it is manditory to slect storage classes while u uploading the objects in s3 bucket 


----------------1 Standared frequently Access(FA) 

- this is usd fofr frequently ccess data 

- it is default Storage Class 

- no retrival charges apply 


- Availability :anytime  is 99.99%

- Durability  : long time is 11 9's 

- Min Object size = 0 bytes

---------------- 2 Standared Infrequently Access(IA) 

- this is usd fof  IN-frequently Access data 

- retrival charges apply

- cheaper than FA 


- Availability :anytime  is 99.99%

- Durability  : long time is 11 9's 

- Min Object size = 128kB

- Min Duration = 30 days 


---------------- 3 Reduce Redundancy Storage(RRS)

- it is Access frequently not accessed data but not critical 

- no retrival charges 

- AWS does not recommend to use this storage class

- cheaper than others 


- Availability :anytime  is 99.99%

- Durability  : long time is 99.99%


------------------ 4 One Zone IA

- infrequently access data but not critical 

- Retrival Charges Apply 


- Availability :anytime  is 99.99%

- Durability  : long time is 11 9's 

- Min Object size = 128KB

- Min Duration = 30 days 

------------------ 5 Intelligent Tier 

-- unknown Access Patterens 

- based on ur access it will shift from one to another classes

- it is only for FA and IA


- Availability :anytime  is 99.99%

- Durability  : long time is 11 9's 

- Min Duration = 30 days 


---------------------- -----Glacier 

- it is used for in-frequently acces data

- Aarchiving Data

- inside Glacier we create vault 

- Vault : container of Archives 

- Archieve : object/ .zip

Note: Archieve can be upto 40 TB

- unlimited number of archives in 1 vault 

- 1000 vaults

- retrival Charges Apply 

------Glacier has Retrival Options 

1 Expedited : 1 to 5 mins 

2 Standard : 3 to 5 hours 

3 Bulk : 5 to 12 hours 

- Min Duration = 90 days 



-- we have here concept Availability and Durability 

- Availability :anytime  is 99.99%

- Durability  : long time is 11 9's -- 99.99999999999%


--- deep glacier 

- Min Duration = 180days 


---------------------------------life cycle management 

NOte : it is possible to move objcts from one storage class to another storage clss automatically by crated Life cycle Rules 

- Life cycle rules can be applied for entire bucket level or for prefix(sub-folder) 


-- Life cycle Rules can be created for current versions and previous versions also 

-- In LCM we have Transistion and expiration 

- Transistion : FA --> IA(30 days) --> Glacier(60 dyas) 

here            oth day --> 30th day --> 60th day 

- expiration : Delete after 365 days 


--------------------Object Lock 

-- permanent lock and certain period lock 

-- if u want to get logs of ur bucket u have to enable " server Access LOgs"

-- create seperate bucket for logs and all logs will stored here 

-- server access logs are Bucket level 

-- it is very hard to read logs when u have more logs (1000000000's)


============================Athena===================


-- to avoid above problem we can use "ATHENA" 

-- it analyze the logs directly from S3 



=================================CORS(cross-origin Resource sharing)============================


-- it is small JSON script 

-- when u host static website in s3 and u have index.html website inside u have puppy.jpg stored in bucket 1 

-- now in bucket 2 u put puppy.jpg, now u are caling from the bucket 1 

-- u wont get coz by default u cant share b/w two other buckets 

-- to avoid this we can use CORS 

-- enable CORS in 2nd Bucket 

-- once u eneble cors it will get puppy.jpg

=============================CRR(Cross-Region Replication)============

-- u have ucket in mumbai and u have another bucket in ingapore 

-- whatever the thing in b1 it will get relicate in singapore also 

-- for this u have to create CRR rules in Bucket 1

-- CRR is not enebled by default 

-- it is BUcket-level

-- CRR can be shared to different Accounts possible 


============SRR(same region replication) ==================


-- if u are replicating same region 

-- it is bucket level 


NOTE : versioning is manditory to have CRR/SRR

-- object level logs can be captured in CloudTrail



---------------------encryption(security)

-- it will be done in 2 ways 

1 in-transist : data is moving by using Https -- for this use ACM

-- ACM : is where u can generate HTTPS certs(in-transit)

2 Data at rest : can be done whle data at rest through KMS 

-- KMS : is where u can cretae Encryption keys for data At rest 



=====================S3 has 3 types of Encryptions ==================

1 server-side encryption : 

- we have 3 types 

------ A  SSE-S3 (aws Managed Key) -- default common encryption method we use ,

- it is used Algorith called AES-256 (advanced Encryption Standard)

- by deafult , bucket encryption is enabled 

explanation : Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in AWS data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects. For example, if you share your objects by using a presigned URL, that URL works the same way for both encrypted and unencrypted objects. Additionally, when you list objects in your bucket, the list API operations return a list of all objects, regardless of whether they are encrypted.


------ B SSE-KMS ( AWS KMS KEY)

explanation : All Amazon S3 buckets have encryption configured by default, and all new objects that are uploaded to an S3 bucket are automatically encrypted at rest. Server-side encryption with Amazon S3 managed keys (SSE-S3) is the default encryption configuration for every bucket in Amazon S3. To use a different type of encryption, you can either specify the type of server-side encryption to use in your S3 PUT requests, or you can set the default encryption configuration in the destination bucket.



------- c SSE-C (Customer provided Keys) 

explanation : Server-side encryption is about protecting data at rest. Server-side encryption encrypts only the object data, not the object metadata. By using server-side encryption with customer-provided keys (SSE-C), you can store your own encryption keys. With the encryption key that you provide as part of your request, Amazon S3 manages data encryption as it writes to disks and data decryption when you access your objects. Therefore, you don't need to maintain any code to perform data encryption and decryption. The only thing that you need to do is manage the encryption keys that you provide.

When you upload an object, Amazon S3 uses the encryption key that you provide to apply AES-256 encryption to your data. Amazon S3 then removes the encryption key from memory. When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key that you provided matches, and then it decrypts the object before returning the object data to you.




2 Client Side Encryption 

Client-side encryption is the act of encrypting your data locally to help ensure its security in transit and at rest. To encrypt your objects before you send them to Amazon S3, use the Amazon S3 Encryption Client. When your objects are encrypted in this manner, your objects aren't exposed to any third party, including AWS. Amazon S3 receives your objects already encrypted; Amazon S3 does not play a role in encrypting or decrypting your objects. You can use both the Amazon S3 Encryption Client and server-side encryption to encrypt your data. When you send encrypted objects to Amazon S3, Amazon S3 doesn't recognize the objects as being encrypted, it only detects typical objects.

The Amazon S3 Encryption Client works as an intermediary between you and Amazon S3. After you instantiate the Amazon S3 Encryption Client, your objects are automatically encrypted and decrypted as part of your Amazon S3 PutObject and GetObject requests. Your objects are all encrypted with a unique data key. The Amazon S3 Encryption Client does not use or interact with bucket keys, even if you specify a KMS key as your wrapping key.


3 In-transit Encryption 

Encrypting Data-at-Rest and Data-in-Transit

To protect data in transit, AWS encourages customers to leverage a multi-level approach. All network traffic between AWS data centers is transparently encrypted at the physical layer. All traffic within a VPC and between peered VPCs across regions is transparently encrypted at the network layer when using supported Amazon EC2 instance types. At the application layer, customers have a choice about whether and how to use encryption using a protocol like Transport Layer Security (TLS). All AWS service endpoints support TLS to create a secure HTTPS connection to make API requests.


----------IMP : -------s3 data Consistency Models 

-- Read after write for PUTS of New Objects 

-- eventually Consistency for OVERWRITES of PUTS and DELETES 



---------==============Pre-signed URL===================== 

if u want to give access for some time only then AWS it has "pre-signed URL" which is like temporary Acess for certain perios of time to the user through a temorary Object URL



================S3 transfer Acceleration============

-- it is billable 

-- it used CDN network to upload very speed in other regions 


=================S3 Requester pays==============

-- in geenral , bucket ownr pay all the S3 storage data trnsfer cost associated to their Bucket

-- with S3 Requester pays buckets , the requester instead of the bucket owner pay the cost of h request and data download from the bucket 

-- helpful if u want to share the large data sets

-- The requester must be authenticated in AWS , so that AWS knows where to charge( in their account), cannot be an0nymous 



================S3 event notifications ===================


-- if someone is trying to adding . doing encryption etc... anything in the s3 u will get notifications 

-- s3 send notifications to SNS 

=================S3 Batch Operations==================

-- if u wnat to perform bulk operations on existing S3 object with a single request 



===========S3 Access Points======================

-- it simplify scurity management for S3 buckets


-- all the people acccesss through the Acces points

-- instead of writing critical bucket policies , u can create access points to each and givw the DNS Names to the users to acces their respective folders in buckets


IMP : A.P can be public(internet) or private(VPC) 




================================S3 practicals=============================


-- when u do Elastic Bean stalk , one buccket is created automatically delete policy first then delete bucket 

-- create bucket , when bucket is private , object is also private 

-- if u create a public bucket ,object is still private ( u can make public if u want)

-- ad some pictures in bucket 

-- objetc lock can be enabled only at the time of creating bucket only , later u can not do it 

-- copy URI is used in the CLI 

-- Pre-signed URL worked for both private and public Buckets 

-- create one public bucket 

-- upload some images and try to acces u cant coz it is still private only 

-- select files --> actions--> make acl public ,now u will gwt image 



============S3 versioning practicals =================


-- enable versioning for bucket 

-- upload same 2 images again

-- once u toggled show versions options t will show u L with image which means the latest 

-- do delete image 1 

-- now select versions and delete the delete marker u will get back u images 

-- this is how u will get back files if u enebled versioning 


-- Bucket Versioning : properties 

-------------IMP : ------------- MFA delete : additional layer of security that requries MFA for changing bucket versioning settings and permaenetly delete object versions . this feature can be enabled CLI only 


-- -- in S3 , it is possible suspend the versioning 

-- once u suspend versioning u cant get back  the files once u dlete 

-- even though u can see delete marker options but u can not get back those files once u delete , when u suspend the versioning 

-- existing objects do not have any impact once u suspend the vesioning 

eg: for image 1 u have enebled version previously and nxt suspend versioning once u dleete image 1 u will get back those image co existing files wont get any effect 


IMP :  for image 3 nad 4 u did not enable versioning 

-- now do enable versioning , now dlete image 4 and go n check enable versions dlete marker u will get back ur image 4 , coz now versioning is enabled 

-- once the enable is enabled u will get back all files , irresective of uploading time with versioning enable time suspended time 





==============Server Access logs======================

-- create seperate buckets for logs 

-- now go to original bucket -> properties--> enabled Server Access logs--> choose log bucket as destination and save changes 



===============================S3 event notifications=============

-- if something is hapen in S3 u will get notifications 

-- S3 event notification will be sent to 3 destinations 

1 lambda function 

2 SNS topic 

3 SQS Queue 


-- select SNS topic 

-- here u will get error like 

  """Unable to validate the following destination configurations""

__-----------------------------Explore topic , solve this 

-- create bucket , go to propeties create new notification choose SNS 

-- open SNS and create topic 

-- edit access policy 

-- Eg

{
 "Version": "2012-10-17",
 "Id": "example-ID",
 "Statement": [
  {
   "Sid": "example-statement-ID",
   "Effect": "Allow",
   "Principal": {
     "Service": "s3.amazonaws.com"
   },
   "Action": [
    "SNS:Publish"
   ],
   "Resource": "ADD-YOUR-ARN-HERE",  ---- in accespolicy resource paste here
   "Condition": {
      "ArnLike": { "aws:SourceArn": "arn:aws:s3:::ADD-YOUR-BUCKET-NAME-HERE" }
   }
  }
 ]
}

-- replace above policy and save changes 

-- now go to bucket and create event notificatin with SNS topic 

-- now try to uplod files , check u get notifications or not ? 

-- Getting ok.......................


====================S3 EventBridge==========================


-- by default s3 events stroed in the Event Bridge , u have to enabled explicitly 



=========== AWS CloudTrail data events ==============

--if u want object level logs u will enebled this 


========================how to setup static website on S3===============

-- download some html template from google 

-- now we want our html website on our S3 

-- create one public bucket 

-- upload folder of all html files 

-- make all files public , -->actions --> make public ACL 

-- once u upload , then enable static web site hosting ,in  demo bucket 

-- access ur webite through url 



============ACCESS POINTS ================


-- exploring topic 


============================S3 CORS practicals ============================


-- create 2 public buckets 

-- create 2 html files 

1 index.html 

2 load.html 


1  

<html>
<head>
<title>AWS s3 Cors Demo </title>
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
</head>
<body>
<h1>AWS S3 CORS DEMO</h1>
<div id="loadDiv"></div>
</body>
</html>
<script type="text/javascript">
$("#loadDiv").load("load.html");
</script>


2

this is load from same bucket 



-- upload 2 htm files and make them as public 

-- enable static web hosting 

-- now try to access link u will get o/p 2 lines calling load.html 

-- now create another bucket 2 (public bucket) and upload only load.html 

-- make file public 

-- copy object url of load.html 


<html>
<head>
<title>AWS S3 CORS DEMO </title>
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
</head>
<body>
<h1>AWS S3 CORS DEMO</h1>
<div id="loadDiv"></div>
</body>
</html>
<script type="text/javascript">
$("#loadDiv").load("https://cors-dem2.s3.ap-south-1.amazonaws.com/load.html");
</script>

-- now upload back index.html(updated) to s3 again and make public in bucket 1

-- make them public 

-- now do refresh the link in browser  

-- u can not able to see load.html content in the browser coz u are calling from the otehr bucket and u did not enable CORS so u are not getting content 

-- now go and create CORS in Bucket2 

-- open 2 bkt --> permissions --> c.o learn more --> copy snippet from document in google 

[
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "PUT",
            "POST",
            "DELETE"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": []
    },
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "PUT",
            "POST",
            "DELETE"
        ],
        "AllowedOrigins": [
            "http://www.example2.com"
        ],
        "ExposeHeaders": []
    },
    {
        "AllowedHeaders": [],
        "AllowedMethods": [
            "GET"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": []
    }
]


-- now do refrsh the link u will get content of load.html 




---------storage class Analysis

-- Analyze storage access patterns to help you decide when to transition objects to the appropriate storage class.


-------------------- Life Cycle Configuration

-- create lifecycle rules acording to your requriment 

-- u can disable rules any time 


----------------------- ----Relication Rules 

-- one bucket files in bk1 , this files replicate to bkt 2

-- go to management 

-- enable versioning is maditory 

--public or private it is does not matter 

-- create 2 buckets in two different regions 

-- go to bkt 1 n create relication rules slect destination path 

-- ask aws to create new IAM role

-- now upload some file sin bkt 1 n do refresh in blt 2 u will get same files in bkt 2 


-------------------------S3 inventory Configuration

-- create I.C 

-- once u create , it will create one report 



------------NOte 

-- s3 bowser download and do ur s3 actions withot go to console every time 





=================================EFS Practicals ================================


-- create EFS 

-- throughput : speed b/w ec2 and EFS 

-- EFS is regional 

-- Replicatio is posssible in EFS , cost calulated how much data is transferred 

-- launch 1 ec2 instances 

-- in S.G add NFS inbound rule 

-- connect instance 1 and follow some commands 

1  sudo -s 

2  yum install -y nfs-utils

3   mkdir efs  --- create one folder 

4   now do mount with folder 


mount -t nfs4 fs-0c90179e0f0b6c46b.efs.ap-south-1.amazonaws.com:/ efs/


explanation :   fs-0c90179e0f0b6c46b.efs.ap-south-1.amazonaws.com  -- u wil get from efs dns name    and efs is folder name 

-- cd efs 

-- now create some files here 


---- now create 2nd indtance in diff A.Z 

-- follow the same steps 

-- once u do ls u will get same filesin diff A.Z zone instances also 

-- create one file new in ec2 1 and check in 2nd ec2 u will get that file 

-- same for deletion also 

-- this is how u do with Efs amd u can also do repliction EFS 



=====================================CLI Practicals=========================

-- wherever u want to access AWS , u have to istall AWS CLI 

-- for windows u need to downlaod --> AWSCLI.msi

-- forlinux u have to use Linux cmnds 

-- configuring KEYS o the insatnce is not recommmended 

-- so launch another ec2 launch aws cli and create Role and give permission 

-- how to acces key? --- Aws configure 

-- o//p : 3 ways table or json or xml 

-- keys are stored in cd~/.aws 

------------praticals

-- launch instance and SSH in S.G 

-- connect instance , follow cmnds 


-- sudo -s 

--curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
-- unzip awscliv2.zip
-- sudo ./aws/install



-- now configure keys

-- aws configure 

-- give keys and region and select table format as o/p 

-- if u want to know where keys stored do 

cd ~/.aws

ls

cat crendentials 

-- u will get ur keys 

-- this is not good to store 

-- try some sample cmnds 

Eg : 

aws ec2 run-instances --image-id ami-0a0f1259dd1c90938 --count 1 --instance-type t2.micro --key-name terraform-key --security-group-ids sg-09756757ec04083da --subnet-id subnet-02c844dbf0a247651


-- as u give op as table mode u will get o/p in table formate 

-- ec2 is created 

-- like this u can do start stop and so many things u want 

-- NOTE : if u want to get data from the resource 

-- type TOKEN=`curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"` \
&& curl -H "X-aws-ec2-metadata-token: $TOKEN" -v http://169.254.169.254/latest/meta-data/


u will get these many options 

ami-id
ami-launch-index
ami-manifest-path
block-device-mapping/
events/
hostname
identity-credentials/
instance-action
instance-id
instance-life-cycle
instance-type
local-hostname
local-ipv4
mac
managed-ssh-keys/
metrics/
network/
placement/
profile
public-hostname
public-ipv4
public-keys/
reservation-id
security-groups
services/


-- if u want to get any one of these data u can add last of cmnd for eg local-ipv4

-- TOKEN=`curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"` \
&& curl -H "X-aws-ec2-metadata-token: $TOKEN" -v http://169.254.169.254/latest/meta-data/local-ipv4


-- now create s3 through console 

-- aws s3 mb s3://6pm-test-demo/ --region ap-south-1

-- check in console u can able to see buckets 

-- aws s3 rb s3://6pm-test-demo/ --force  == to remove bucket 

-- it is not recommended to do this way coz keys are stored in ec2 itself so u can not do this way 

-- so create role and give permissions and run ur cmnds without keys 




================================AWS Transfer Family=========================

-- Fully managed Service for file transer securely 

-- Suport SFTP,FTP,FTPS(FTP over SSL) 

-- data can be transferred in and out of s3 buckets and EFS 

-- 
genlly we are uploaing our files in S3 through the console  but we are doing through the http , 

-- now w r going to transfer files securely from ur laptop 

-- for this we have to install software called "FileZilla" which is used to transfer fis very securley 

-- b/w ur laptop and s3 or EFS we have to create One "Transfer Server" , this server allows us to do  SFTP,FTP,FTPS, we can choose any one , and we hav to create one user in Transfer server 

-- in FileZilla u can login as with user name 

-- once u create server it will give u endpoint 

------------------practical

-- create public bucket 

-- create one bucket policy through policy generator 

eg :   

{
  "Id": "Policy1704630072890",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1704629976563",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket",
        "s3:PutObject"
      ],
      "Effect": "Allow",
      "Resource": ["arn:aws:s3:::transfer-dem",
                       "arn:aws:s3:::transfer-dem/*" ],
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": "49.205.119.80/32"
        }
      },
      "Principal": "*"
    }
  ]
}



-- now create transfer server 

-- open aws transfer family in console 

-- create server , selete SFTP --> Srvice managed --> select as per ur requriemnets 

--now add user in server 

-- select server c.o add user --> create one role in IAM --> truste entity = ec2 --> use case = Transfer (imp) --> give s3 full access --> create role 

-- go to puttyGen n create SSH keys(mouseover)  and save private key with .ppk 

-- now download Filezilla 

-- open filezilla --File(top) --> site manager --> new site --> copy end point of server --> selet SFTP n paste endpoint of server and give port number as 22--> logon type : key file --> give ur private file u downoaded -- connect 


-- once u coonect u jst drag do upload ur files u want and go check in s3 in console --- getting files 

-- this is how u can transfer files throug SFTP from ur lap to s3 or EFS 



===========================AWS - Storage Gateway Service==========================


-- it combination of on-premises and AWS Storage , so it is called " Hybrid Storage"

-- it is used to transfer the data from on-premises to AWS vice versa (S3, EBS,Glacier and FSx) 

-- ur lap is also called on-premises 

-- it is not possible to mount storage service(s3,efs,glacier,FSx) to ur on-premises but by use of storage gateway u can do this 

-- there are 4 types of Storage gateways 


1 if u want to use "s3" create "file gateway"


imp : File Gateway supports "NFS" for LINUX and for WINDOWS it use "SMB"(server message Block)



2 if u want to create "EBS" create "volume gateway" 

Imp : volume gateway we ahve 2 types 

1 cached volume : to increase the performance ,it uses protocal "iSCSi"
you store volume data in AWS, with a small portion of recently accessed data in the cache on-premises. ...

2 Stored Volume : you store the entire set of volume data on-premises and store periodic point-in-time backups (snapshots) in AWS.


3 if u want to create "Glacier" create "tape gateway"

-- no need to maintain physical tape 

-- AWS provide VTL(virtual tape library) to take backups , it uses protocal "iSCSi"



4 if u want to create "FSx" create "FSx gateway" 



--------------------prac

-- we do not have on-premises ,so consider ur lap as on-premises and VM as Ec2 and install Storage gateway appliance(agent) on on-premises

-- this is chargable 

-- go to soage gateway and create s3 file gateway 

-- Platform options = ec2 

-- customize ur settings 

-- c.o launch instance , automatically AMI is take here 

-- select Instance type is = m5.xlarge

-- add additional volume of 150 GiB of gp2 

-- c.o checkbox and nxt 

-- copy IP of instance and give in nxt step as a connection 

-- now on-premises and file gate way now connected 

-- activate and configure 

-- now ceate s3 private bucket 

-- now file share we have to create , if u open gateway u willhave options like what to creat now we have files so create file share 

-- all the data come to the file share and this data stored in AWS S3 

-- c.o customize configuration , in step 3 u can give accesss client to all (0.0.0.0/0)

-- open file share u will find "mount point"

-- launch t2.micro instance for testing 

-- connect nstance 

-- sudo -s 

-- yum install -y nfs-utils

-- mkdir filesystem

-- go to file share and copy mount point for linux 

-- mount -t nfs -o nolock,hard 172.31.1.243:/store-gate [MountPath]/

-- cd foledername

-- now indirectly iam in S3 only if i put any data and stored this data in S3 

-- do df -h n see s3 bucket is there so u r doing mount 

-- create some files 

-- check in S3 ur file will store in S3 automatically 

-- the file u created all go through Storage gateway appliance(agent) --> storefile gateway--> S3 


-- delete all resource 

-- delete additional volume also it has 150 GIB 





=======================================RDS==============================

-- RDS is REgional 

-- in RDS we called RDS DB instance 

-- it has Endpoint 

-- It is "PAAS", u can not login but u can connect 

-- it hs 2 topics mainly 

1 Read Replicas 

2 Multi AZ


-- AWS RDS has 6 Engines 

-- generally , how u connect to data base?

we need Hostname /endpont 

- usrname 

- passwd

- port number 

-- ndise db engine(whole database) we can multiple db's 

-- now , there are some aplications only for the read-base purpose , so so many aplin are connected to the engine it will get overload 

-- so do create seperate Read Replica , it will us for only Reading -purpose 

-- so all read applins are getting conected to this read -replicas and fetch the data from the replica 

-- we have main server( Master), generaly it has a role , if it does not have read replicas then it is only "instance" , if u create Read replicas from the master then it will beome "primary"

-- read replicas are connected through Endpoints 

-- Read replicas can be in multiple Regions 

-- all the rights will be done in Master server only 

-- when some one writing in the main server ,paralley it does not update in the read replicas so it is called " Asynchrous"

-- RR is used for Increase the performance 

-- but it is not for High availability 

-- Max 5 Read Replicas 

-- ReadReplicas have their own endpoints 

-- endponts are provided by the AWS 

-- u can promote RR to Nrml stand alone DB machines, if u create RR from this it will become Primary 

- u can enable M-AZ for RR also . charges are double 

-----------------------MUlTI_AZ / Cluster 

-- Mutli AZ is used for the High availability 

-- if u enable M-AZ , one more DB instance crated , we can not see that instance , it is handled by the AWS 

-- it is "SYNCHRONOUS" 

-- u need to pay Double amount when u have enabled Multi-AZ 

-- if something happens to ur main DB server , the failover happens here , the requests will send to backend server 

-- endpoint won't change even in failover time 

-- DB operations will not have failover(for eg :some one delete table it wont get failover , tese are responsible by us )  , anything realted to network, servers etc will have fail over 


-- RDS DB instances can be reserved 

---------------RDS Features 

-- backups are called Snapshots 

-- if u want to take backup of whole Engine it is called "Snapshot" 

-- Snapshots  will takek 2 types 

1 manully 

2 Automatic(schedule)

-- if u want to take only one data base inside engine u have to do manualy write scrits and AWS do not have service for the DB level Operations backups 

-- DB level Operations (tables,SP,DB-level backups, Scripts, Fns,Security etc) are handled by Customer 

-- dB instance Level (instance Backup(Snashots), configurations, restorations , capacity etc)

-- Operations is handled by AWS/Platform is handled by AWS 

-- u can also do Encryption

-- we have storage/ volumes --> gp2,io1

- instance type --> Db insatcne type --> t2.micro 

-- SG/VPC/subnet

-- performace insights (Dashboard)

-- backup retention period = Max 35days , default 7 days ( Automatic) 

-- No retention priod for Manual Backups, u need to delete it manually 

-- Snapshots can be exported to S3/ restore it from S3 Also 

-- Snapshots can be copied from from one region to another region 

-- shared from one accnt to another accnt 

-- U can do Scale up DB instance but not on FLY mode ( we need to stop the DB instance, Downtime is Requried) 

-- we have ASG on "storage Level"  min 100GB n max 1000GB


-- AWS proporetary Engine is "AURORA" , own product 

-- is is compatible with MySQL and POSTGRES

-- It is serverless/server base based 

-- it is high chargable 

1 aurora 

Aurora MySQL is Amazon’s enterprise-class MySQL-compatible database.

Aurora MySQL offers:

Up to five times the throughput of MySQL Community Edition

Up to 128 TB of autoscaling SSD storage

Six-way replication across three Availability Zones

Up to 15 read replicas with replica lag under 10-ms

Automatic monitoring with failover

2 Postgres

Aurora PostgreSQL is Amazon’s enterprise-class PostgreSQL-compatible database.

Aurora PostgreSQL offers:

Up to three times the throughput of PostgreSQL

Up to 128 TB of autoscaling SSD storage

Six-way replication across three Availability Zones

Up to 15 read replicas with replica lag under 10-ms

Automatic monitoring with failover





------------------------------AWS RDS PROXY

-- it is fully managed data base proxy for RDS by the AWS 

-- it is chargable 

-- allows apps to pool and share the DB connections established with the data base 

-- Improving database effiency by rducing the stress on the data base resource(Eg CPU, RAM ) and Minimize open connections(and timeouts)

-- Serverles,AutoSclaing, HA

-- Support MYSQL, POstgres, MariaDB, MSQL and AURORA 

-- Secret Mangaer is a AWS Service ehre u can store all Scrects(keys, usernames,passwords etc) 

-- RDS Supports SSM to get credntaials 

-- RDS proxy is "never publicly accessible (must be accessed from VPC) "


Summary: RDS proxy will allow ur appns to ool and share the DB connections established with the Db instead of having every single app connect to rds instace they wil be instead connecting to te proxy and proxy will pool these connctions together into less connections to the RdS DB instance 


-- Everything should be inside the VPC 


=================================RDS PRAC========================================


-- create Db with free tier configuration , MYsql without backups 

-- create repelica option grey out coz

Backups for your instance are currently disabled (retention = 0). Please enable backups before attempting to create a read replica.

-- by deafult the role of data base is Insatnce 

-- if u want to edit anything --> go to modify n do changes u want

-- read replica is only read purpose 

-- u can promote to stand alone  

-- once u create snapshots from data base engine , from snapshot u wl create new engine but u can not give same name for db engine 

-- for eg if someone delete table u can get restore from the napshot but the whole engine u should do backup coz db operations are resonsile by us not AWS 

-- go to SG default and add ur engine port number , mine is MYsQL 3306 

-- copy endpoint of RDS 

-- open mysql workbnch and try to connect 

NOte: if u are using company wifi or some private internet , try to connect to ur mobile hotsopt u will get connect 

-- u can also cange verison of snapshot engine with jst one click 

 NOte : when u want to restore from the S3 ur engines u can get only 2 engines 

AUrora and MYSql 


====================================ELASTIC CACHE=======================

-- it is developer resource mainly

-- it is regional 

-- it is also data base only 

-- In-memory data base caching Service 

-- Cache : all frquently accessed data is stored at this place 

-- it is used for Read Purpose 

-- it can handled Session data(cookies) 

-- it supports Encryption ( IN-TRANSIT and DATA at rest) 


-- it supports two Engines 

1 Redis : permannent

- it supports HA, failover and backups

- Data is Persistent 


2 MemcacheD --temporary 

- it not supports HA, failover and backups

- Data not Persistent 



-- if the application hit cache it wil give data without going to RDS 

-- if Cache is not have data , appn go to RDS and get data whatever the data get from the RDS the application writes in the Cache coz it is missed data , so nxt tym if u search for same data it will get from cache not from RDS (this will do developer) 

-- it use to improv the performance 

-------------------------------Developer Strategies

-- they are 2 type sof stragies

1  Lazy Loading : load the data , when it is necessary 

if Cache is not have data , appn go to RDS and get data whatever the data get from the RDS the application writes in the Cache coz it is missed data , so nxt tym if u search for same data it will get from cache not from RDS


2 Write Through : write parallelly both data basess (DB and Cache) , in this 100% similar data on Both RDS and Elastic cache 


--------------------------------------REDIS

-- it hs 2 types of modes 

-- it is like NoSQL database 

1  Cluster Mode Enabled  

-- Cluster = Collection of Shards

-- Shard = Cllection of Nodes/Server

-- Each Shard has 6 nodes = 1 Primary Node , 5 Replica Node

-- u can have 500 Shards per cluster 


2  Cluster Mode disabled

-- it has ony one shard = 1 primary and 5 replias 

-- 




-------- imp : Service Quotas ia AWS service where u can find the soft limit of the AWS Resource 

-- if requrd u can increase the soft limit 





=================================AWS DynamoDB===================================

------------------------------Basics OF Dynamodb 

- stored on SSD Storage 

- sread across 3 geographically distinct data centers

- eventuall consistent Reads(default) --- ECR 

- Strongly Consistent Reads ---- SCR 





-- this is purely developer service 

-- DdB offers "push button" scaling , meaning that u can scale ur database on the fly, without any downtime 

-- it is serverless 

-- Table is collection of Attributes(Coloumns) and Items(Rows) in Amazon dynamo db 

-- Primary Key is always unique and not null 

-- in  Dynamodb Primary Key is called "partition Key"

-- in Ddb , initially u can create a table with single coloumn and tht should be Primary key 

-- in Ddb , p.k is manditory while creating a table 

-- Composite Key = Primary Key + Sort Key 

-- Sort Key is optional 

-- if u have a primary key for a single coloumn = Duplicate values are not allowed 

-- if u have p.k + sk = duplicate values are allowed 

-- Ddb data stroed in JSON format





----- "Dynamo Db Streams" = what ever the item level changes wil be captured 




----------------Indexs

-- thse indexes ae writes by developers 

-- 2 types of Indexes

1 LSI(ocal secondary index) = partition key + Any Coloumn as Sk

2 GSI (gobal secondary index) = Any coloumn as pk + any coloumn as SK 

Note : -- lsi can be created only at the time of creating the DynamoDb table, later u cannot Modify/delete the LSI 

-- GSI can be Created, modified, delete any time 




------------- provisioned Capacity Units 

-- 2 types 

1 RCU ( read capacity units) 

2 WCU (Write Capacity Units )

-- Ddb performance depend upon RCU and WCU 

-- 1 RCU = 5.2 Million Reads it will do 

-- 1 WCU = 2.5 million Writes 



-- by default DDb giving 5 RCU and 5 WCU 

-- For ECR , 1 RCU = 2 Reads per second of 4KB size

-- For SCR, 1RCU = 1 read per second of 4KB Size 

-- 1 WCU = 1 write per second of 1KB Size 




SQL VS NOSQL 


SQL :

-- it is generally used in RDBMS 

-- structures data can be stored in tables 

-- The Schemas Are Static 

-- schemas are rigid and bound to relationships 

-- Helpful to design complex queries 

-- here we call tables , rows and coloumns 

-- eg : Mysql,oracle,sqlite,Postgres and MS_SQL 

-- it has different types 

Relational and Analytical(OLAP) 


NOSQL :

-- it is generally used in NON-RDBMS 

-- Using JSON data, unstructures data can be stored 

-- The Schemas Are dynamic

-- schemas are no-rigid , they are flexible 

-- no interface to prepare complex queries 

-- here we call collections, collections has Documents 

- Eg : MongoDB, BigTable, Redis, RavenDB, Cassandra, Hbase, Neo4j and couchDB 

--it has diff types 

Coloumn-Family , Graph , Document and Key-value 

------------------

-- our "DynamoDB supports "Document and key-value(JSON) pair" 


---------------Provision Capcity examples 


-------------------------------Example For RCU 

eg : if u have a table and u want to read 100 items per second with Strongly Consistent Reads(SCR) and ur items are 8KB in Size, u would calculate the required provisioned capacity as follows ,

-- we kanow the basic of 



-- For ECR , 1 RCU = 2 Reads per second of 4KB size

-- For SCR, 1RCU = 1 read per second of 4KB Size 

-- 1 WCU = 1 write per second of 1KB Size 

-- to solve this 

Sol : here it is asking 8KB size , what is our size? for SCR is 1RCU = 1 read per second of 4KB Size 


so 8KB/4KB = 2 capacity units 

in the question it is asking foor 100 items per second to read , so 

2 read capacity units per item * 100 reads per second = 200 / 1 = 200 read capacity units 

NOte : if it is ECR then 200 / 2 = 100 Read cpacity units 


----------------------Examle for WCU 


-- if u have a table and u want to write 50 items per second and ur items are 4KB in size , u wuld calculate the required provisioned caapcity as folows 


sol : 4 KB / 1KB  = 4 WCU 

     4 WCU * 50 Units per second = 200/1 = 200 write units 





-------------- for eg u have question like 

Q : 10 Strongly Consistent Reads per seconds of 6 KB each 

sol: here we do not have 6 we have only 4 and 8 KB so,go for near (only up) so 8KB 


ans : 20 RCU 



--------------Dynamo db table prac


-- Ddb n cnsole create table 

-- customize settings -> Ddb std

-- provisionsed



--- Global table : used to replicate ur db 

note : To create a replica, you must set your table's and index's throughput capacity to auto scaling, or change the capacity mode to on-demand.


-- DynamoDB stream details = Capture item-level changes in your table, and push the changes to a DynamoDB stream. You then can access the change information through the DynamoDB Streams API.


-- with the use of sort key , u can do reuse primary key ( name) multiples times but u can not do use same sort key values




==============================CLOUD-FRONT=================================================


-- it is global servie 

--  here u have s3 bucket which have static website in it 

-- if ur user in U.S , the user connect to our website through edge Location not directly to our website , if he cnnect direct from our region the latency will be high 

-- we need to setup CloudFront it create distributions and origin is =S3 bucket 

-- rom edge Location to website the data is stored through the CDN , it is usper speed managed by the AWS Network

-- once u create CF , it give nasty URL 


-------------------Prac 

-- open S3 and crate private Bucket 

-- as u cate private bucket , no one can access ur url through the S3 

-- Only access through by Cloud-Front directly coz, it is privtae bucket and we did not enable Static hosting also 

-- go to CF in console 

-- create ditrubtion on CF 

-- Origina Domain = load balancer / S3 -- these are the places wher u can host ur applications 

-- OAC --> Create control settings --> do nt change any n create 

-- it is created access from S3 

-- Compress objects automatically : CloudFront can automatically compress certain files that it receives from the origin before delivering them to the viewer. CloudFront compresses files only when the viewer supports it, as specified in the Accept-Encoding header in the viewer request.


-- Default root object - optional = index.html 


----- once u crate distrubtion , the S3 bucket policy wil gwt generated copy that policy and paste in bucket policy 

-- now ur appn is getting deployed all over the world 

-- through the CF url cutomers wil able to cnnect ur webiste through the edge locations 

-- onc eu change the content of ur website and do ulod again n if u do refresh u wont get new content 

-- u have too do "invalidate the Cache" 

-- go o CF and create invalidation for /index.html , it will get latest file from the S3 and give latest content to customers 

-- By default, CloudFront caches files in edge locations for 24 hours. 




==============================Route 53===================================

-- it is global service 

-- it is DNS(DOMAIN NAME SERVICE)  Service in AWS

-- it keeps track of all hostnames and IP's

-- it converts IP to Host and Host IP to IP  

-- normally ur beowser requires 2 things 

1 Host name 

2 IP 

-- the request wil go to LOcal DNS --> Root Name Server -->> Top level domain --> Name server--> SOA 

-- we only have to take care about server--> SOA  only this is configure in R53 


-----------------------R53 Features

-- Domian registration 

-- DNS routing 

-- Health checks

-- Routng policies 



-- in R53 , we should create "Hosted Zones"(container of Recoreds) first 

-- Hosted Zone (Domain name = subbu.com ) are same 

-- 2 types of H.Z 

public 

private 

-- subbu.com -> elb gives nsty url --> ec2(it has appn)

-- whenever u type subbu.com in browser -> R53--> Hoste Zone --> ELB-->Nasty Url--> Ec2 Instance 


--- whenevr u creat "Public Hosted Zone " 2 records will create automatically 

1 NS(Name Server) Record = Pool of servers ( ~ it give 4 Servers) ,it identifies the server(website) 

2 SOA Record : Admin for Hosted Zones ( it has IP address) 


-- NS and SOA are default records , automatically created and managed by AWS 

-- u can purchase domains in 2 ways 

1 R53 

2  3rd party (go daddy) 


-- if u purchase in R53 ,it wil create hoste zones automaticlaly and  NS and SOA records created automatically 


-- whenever u purchase domain 3rd party domain registery , u should connect t AWS ,'coz ur request is here going to GO-daddy so u should connct to AWS 

-- HOW to conect ? 


-- once u create hosted zones in R53 ,the NS records wil crated (4 servers) , these servers u should updated in th Go-daddy , once u updated requests are redirected to AWS 


-- it has some latency 


-- NS and SOA records cannot be deleted 



---------------------------Route53 Records--


-- we have A Record , AAAA Record , Cname recor , Alias Record  , MX Record 

-- Most common use is A record + Alias Record 

1 A Record : URL to IPv4


http://subbbu.com -- > it reach R53--> Hosted zone (subbu.com) --> records it contains subbu.com --> Ec2 instance IP 


URL --> ipv4


2  AAAA Reacord : URL to IPv6


3 Alias Record : URL to Any Resource :

http://subbbu.com -- > it reach R53--> Hosted zone (subbu.com) --> records(subbu.com --> ec2 ip or elb nasty url)

- url to ANY Resource 



4 CNAME Record : URL to URL : 

http://subbbu.com -- > it reach R53--> Hosted zone (subbu.com) --> records (subbu.com-->ELB Nasty URL)

URL to URL 





--- here 

-- http://subbbu.com  ----- Main Domain / Naked domain / Zone Apex Record 

-- admin.gopi.com , web.subbu.com ----- called Sub-domains 


-- CNAME Records are bilable , where as Alias Rec are free 

-- for Naked domain/Main domain we can not use CNAME , instead use Alias 

-- for sub-domains u can u can use CNAME

-- NOte: Always choose Alias over CNAME 



====================================ROUTING POLICIES

1               Simple Routing Policy

-- when u search for soemthing it wil goto R53 

http://subbbu.com --> r53 --> hosted ones --> Records --> ELB DNS name 

-- here we are doing simple routing so it is called "simple Routing Policy"

-- it does not have Health Checks 

-- if any down happens of websites u won't get any response from website 



2               FailOver Routing Policy : 

-- u have to create 2 records for this policy 

eg : 

  subbu.com --> ELB DNS Name(Mumbai) -- primary record 

  subbu.com --> ELB DNS Name(ireland) -- Secondary record 


if one record get down , the request will automaticall go to second record 

-- for this u have to setup 2 sites in different regions for high availability if downtime of websites happens

-- if u have maintainnece page ,u go for S3 , u can redirect to S3 ur request  

-- it has health checks



3                  Geo-Location Routing Policy;


-- for eg u have cutomers all over the world if they are in china , japan , india, austria etc

-- here if some one is from japan , search for http://subbu.com but he wants in Japanese language 

-- so we have application in each region , we donot have other options , so in each region we do have  appn in region seperately , in this case CF is not work 

-- if user search for something from japan , he will get reply from japan only 

-- how to do? 

we have to create records for each regions eg : if u have 5 regions then create 5 records 

http://subbu.com-->Mumbai ELB/IP
http://subbu.com-->sydney ELB/IP
http://subbu.com-->us ELB/IP
http://subbu.com-->ALSKA ELB/IP
http://subbu.com-->japan ELB/IP


-- R53 will identify user's/request location automatically and redirect to the correct record 



4                Latency Routing Policy 



-- u have different Regions , u have appn in every region seperately

-- if he made any request it does not matter about the regions 

-- which regions give low latency from there resonxe will get 



5       Mutli-Value Routing Policy (weighted -Routing Policy)

--  Same as Simple routing Polcy but it has Health Checks 

-- share the traffic based on the traffic 


============================R53 prac=============

-- to do this buy domain in R53 registry or do purchase from any other 3rd party services 

-- i Bought from NameCheap 

-- go to your domain --> manage domain --> custom domain in drop down menu

-- now create one ec2 instance with the user data 

-- make sure you are using linux 2 version 

#!/bin/bash
# Use this for your user data (script from top to bottom)
# install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html


IMP NOTE : If u r getting apache test page I instead of ur content) no need to worry , just follow below things -- Geek Dairy

-- connect Linux machine and login as super user sudo su

-- Method 1

-- removing/renaming Welcome Page

mv /etc/httpd/conf.d/welcome.conf /etc/httpd/conf.d/welcome.conf_backup

-- Make sure that Apache is restarted (as root) with the command:

systemctl restart httpd


-- Method 2

-- allow Indexes in /etc/httpd/conf.d/welcome.conf

-- Without an index at the DocumentRoot, the default Apache Welcome page will display unless /etc/httpd/conf.d/welcome.conf is modified to allow Indexes. Edit /etc/httpd/conf.d/welcome.conf to allow Indexes.

-- Comment the Options line (add a # mark) in /etc/httpd/conf.d/welcome.conf as shown below:

vi /etc/httpd/conf.d/welcome.conf
<LocationMatch "^/+$">
#   Options -Indexes
    ErrorDocument 403 /error/noindex.html
</LocationMatch>

              or 

you can enable Indexes by changing the – to a +

vi /etc/httpd/conf.d/welcome.conf
<LocationMatch "^/+$">
    Options +Indexes
    ErrorDocument 403 /error/noindex.html
</LocationMatch>

-- systemctl restart httpd

-- thats it !


-- now do create another ec2 with same userdata in 1b availability zone

-- make sure you have to give HTTP in SG 

-- now create one Load Balancer (application load balancer) 

-- it is not good to give nasty url of LB so do make over for this link 

-- go to R53 service 

-- You have to create one hosted zone in R53 

-- if You purchase domain in AWS itself automatically it will create Hosted zone for you 

-- if u buy some other places u have to create ur hosted zones 

-- create hosted zone, the name is  ( it should be same ur domain name)

-- mine is  subbucloud.lat

-- create hosted zone with public type 

-- once u created hosted zone it will create 2 Records automatically Ns and SOA

-- NS have 4 servers , u should give these servers in ur custom domain (3rd party) 

-- update ur Name servers in ur custom domain 

-- now create records to do make up of ur url 

-- here u can go with subdomian or with naked (main) Url 


1 simple routing policy

-- type sample is sub-domain and record type is A

-- choose Alias Always on , if u do not on u have to give ur IP but we have LB so choose Alias

-- in route traffic section 

select   alias to application and classic load balancer 

select AZ , LB and Routing policy 

-- create record


2  FailOver Routing Policy

-- FailOver it means in simple words if ur website is facing down period in one Region then , automatically the Load balancer will shift the request into the another another region 

-- in Genaral we have deployed our application in 2 regions for high availability , if one region goes down automatically the request shift to another region through the load balancer 

-- for this we have 2 primary site and secondary site , if primary site is not working then it will redirect to secondary site 

-- we know how to create load balancers in different regions but now we will try with the maintainance page which is stored in S3 

-- create one html files add some content related to the maintainance and do store in the s3 bucket (IMP: ur record name and bucket name should be same  then only it works )

-- make sure ur bucket name is ur domain for easy understanding purpose (ACL is enabled for bucket)

-- upload html file and make file has Public and enable static weh hosting ( in index document u have to give ur index.html extension like maintainace.html

-- create records for load balancer and s3 

-- for s3 record u have to slect end point as s3 website endpoint in dropdown below 

-- check ur url is working 

-- now stop the primary site and observe the changes 

-- if ur primary site is not working then it will shift to secondary site so , here u will get mainatainanace page , through the health checks the LOad balancer know that primary site is down so , it will shift traffic to the nxt site

-- getting maintainance page successfully 


-- u can also do same for geo location policy



========================================VPC(Virual Private Cloud)==================

-- it is like a virtual Data center of cloud 

-- Vpc is Regional, MaxVPC's per Region is 5 

VPC 
Internet Gateway
Public and private subnets  ( 200 subnets per VPC)
NAT Gateway
Route with Route Tables



-- Internet Gateway : it provide internet to the VPC , creat IG and attach to VPC

-- Public subnet : it is exposd to the internet , public subnet raffic is roouted to Internet Gateway 

-- Private subnet : it is not exposd to the internet , private subnet raffic is roouted to NAT(NEtwork Addres Translator) Gateway 


-- The job of NAT Gateway is provide internet to the Private Subnet and convert Private IP to Public IP 

-- The NAT Gateway Should put in Public subnet 


-- Routing:  they r 2 types of R.T

1 Public RT : all traffic is routed to IGW , public subnet is associated

2 Private RT : ALl traffic is routed to NAT, Private Subnet is Associated 


--------------------Steps to create our own VPC

1 crate VPC

2 Create IG and attachto the VPC 

3 Create Public and private Subnets

4 Create NAT Gateway(in public subnet) 

5 Create public RT --> all traffic is routed to IG , public subnet is associated 

6 create Private RT --> All Traffic is routed to NAT,Private Subnet is Associated 

7 Create a new Security Group, Allow RDP/SSH 

8 launch ec2 in Public subnet (Bastion Server) and another in Private Subnet 

9 try to connecting to private server through Bastion server 


NOTE : IG and NAT(Managed by AWS) Gateways are "Services not Servers" 




--- from ur laptap , u can directly connect to public internnet, but u can not connect private subnet directly 

-- to connect private subnet from internet(from latap , not from company)  , u have to launch one Ec2 in Public Subnet which is called "Baston server / Jump server" ( it is only for learnig purpose not real time scenario) 

-- first login into baston server and connect to private subnet 

-- in real scenario ur company is private network , u are using VPN Connection from company to AWS , so u no need to login into baston server , u can directly acces private servers 

-- if u want to connect client private subnets u first need to login into client's Baston server 






Question : one person came to u and ask , i have ec2 instnce in private subnet and i do not want internet access to my private subnet what u do? but he wants to Access AWS services from private subnet

SOln:

-- By default 3 sunets are there , these are allocated to each AZ , all these are public subnets only ( u rable to login into ec2) that's why it is public subnets 

-- we do not have NAT for Default VPC


but he wants to Access AWS services from private subnet ? 

-- in AWS We have VPC ENDPOINTS : it is use to Access only AWS services without NAT Gateway



----------------------------CIDR(Classless InterDomain Routing) 

-- private ip series starts with eg: 172.98.90.2/16


--  172.98.90.2 = Base IP 

-- /16 = SubnetMask

-- eg: u can create 3 subnets in VPC

192.168.1.0/24 = s1  -----> it it is routing to IG , so it is called Public subnet (1a AZ)

192.168.2.0/24 = s2 ------> it it is routing to NAT , so it is called private subnet (1B)

192.168.3.0/24 = s3 ------> it it is routing to VPCENDPOINTS , so it is called private subnet (1c)


-- for private purpose it starts with 192, 10, 172 

-- rest all are for public IP's

-- 1 subnet should be in one AZ at the same Time 

--- 1 AZ can have multiple subnets  


------------------------------------- what is SubnetMask :this refers to how many IP's that u get inside subnet

-- we have formula 

-------------------------------   2^32-n


for eg /24 then 


2^32-24 == 2^8 = 256 IP's u wil get , u can launch 256 ec2 u can launch 


NOTE : u have to remove 5 IP's from each subnet maks , here u will get 251 


- 5 IP's are reserved for each subnets 

.0 = Network purpose 

.1 = Router

.2 = DNS 

.3 = Future purpose 

.255 = BroadCasting 


/32 = pnly one IP



-------------------------------------------steps to create VPC with CIDR

1  crate VPCwith CIDR (192.168.0.0/16)

2  Create IGW and attach to the VPC 

3  Create Public subnet(192.168.1.0/24)

4  Create Private subnet(192.168.2.0/24)

5  create NAT Gateway (in public subnet, and also we have to provide EIP for NAT) 

6  Create Public RT ---> al traffic is routed to IGW ,public subnet is associated 

7  create Private RT --> all traffic is routed to NAT, rivate Subnet is Associated 

8  Create a new Security Group , allow RDP/SSH 

9 Launch EC2 instance in public Subnet (Bastion Server) and Another in private Subnet 

10 Connect to BAstion first and then into the private server 



======================================Prac(VPC)==============================



-- create VPC with CIDR 192.168.0.0/16


-- One IGW attach to only one VPC at the same time 

-- create IGW , and attach to vpc that u have created 

-- create public subnet , give subnet CIDR as 192.168.1.0/24

-- create private subnet , give subnet CIDR as 192.168.2.0/24

-- now Create NAT Gateway, it should be in the public subnet only 

-- create RT , once new VPC created a Default RT created automatically 

-- create RT public and private 

-- now go to public RT and add Route--> edit routes --> add route --> 0.0.0.0/0--> IGW  and also associate public subnet for public RT 


-- do same process for Private RT with private rulels

-- now create ur own SG's with ur VPC

-- launch 1 instance , in public subnet 

-- launch 1 instance , in private  subnet and disable Public 

-- now connect to public subnet instance , sudo -s

--    try connect to private server 

--  to conect private server , u just copy SSH Client id and paste in public istance 

eg : ssh -i "terraform-key.pem" ec2-user@192.168.2.31


-- create one file to store .pem value in the public instance 

-- vi terraform-key.pem and paste content of .pem file 

-- we do ot have read permissions for this , so create prmisions 

     chmod 400 terraform-key.pem

-- now try to connect --> Yes

-- once u connect succesfully ,it will connect to the private server 

previusly it was root@ip-192-168-1-129 ec2-user 

-- once u connect to private server u will redirected to the private IP of private server like    
[ec2-user@ip-192-168-2-31 ~]$ 


---------------------ENDPOINTS

-- now u are in private server 

-- do sudo -s

-- this private server in the private subnet and Associated to Private Route Table , in Private RT all traffic is routed to NAT Gateway, so it has internet access 

-- test whether it has internet or not 

--  yum install -y git ,it installd so , it has internet

-- but i do not wnat internet access , so go to Private RT and delete entry for NAT 

-- not able to login 

--   i want only access to AWS service from this linux machine without internet access 

-- so first install AWS CLI 

-- we r in private server so enable NAT access to download AWS CLI in Private RT 

-- curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

-- unzip awscliv2.zip

sudo ./aws/install

-- follow above  steps to install 

-- now i do not want to give keys in the ec2 instance itself 'coz it is not safe to give keys here so , attach role to this insatce 

-- IAM -->roles --> Aws service (TE) --> user EC2 --> give full access of which service u want to access , (s3) --> create role 

-- attach role to the private server 

-- go to private server and try to access bucket 

-- create bucket in linux 

      aws s3 mb s3://vpc-bucket --region ap-south-1

-- u can able to create bucket it has NAT mens full internet access , check in s3 bucket is created 


-----------now i do not want intrnet Acces , so delete NAT entry in Private RT 

-- now we do not have internet Access but i want to acces the AWS service like s3 

-- create endpoint in the console 

-- ENDPOINTS are 3 typrs 

1 Interface-Endpoint : it uses ENI(elastic network interface) , it has private link 

2 Gateway Load Balancer :  it uses ENI(elastic network interface) , it has private link 

3 Gateway Endpoints : recommended , it works with Routing tables 

-- use Gateway Endpoint 

-- search for s3  and use Gateway Endpoints --> se;ect VPC and attach RT to private Routing Tables 

-- EndPOINT use AWS inernal network toacces the services 

-- go n check in Private RT it wi create one oute for ENDPOINT ( pl-78a54011)

-- pl = prefix list = group of network ranges  

-- we have created ENDPINTS , now try to Access 

-- now try to create bucket 

aws s3 mb s3://vpc-bucket19876 --region ap-south-1

-- u will able to create without NAT and internet 

-- aws s3 ls 

-- it works only to connect for AWS Services only 




==========================================VPC PEERING==============================

-- create pering can be done in same region , same account and different account also 

-- create 2 vpc's , one in mumbai and one in ieland regions 

-- Two VPC CIDR should be UNIQUE 

-- u can connect to ur servers n vpc which is another region through the VPC Peering 

-- create 2 vpc in different region 

-- follow above steps for creating all requriments IGW ,create public nad private subnets, NAT , Public and private RT's ssociate route tables and Subnets and SG and 2 instances  for vpc 

-- do the same things in other region CIDR 192.169.0.0/16, u can same region or u can do in another account also ( no need to create public instance(baston) , create private subnet only 

-- once u do all setup 

--  go to mumbai region connect BAston server first ,sudo -s

-- create one file to store terraform-key.pem value , vi terraform-key.pem

-- copy SSH Client link of private server 

-- chmod 400 terraform-key.pem

-- now u are able to connect private server from the baston server 

------- now do sudo -s

-- now frm mumbai u should get connect to the ireland 

-- go and copy SSH CLient of ireland private instance 


-- now go to mumbai location and open peering connection , once u created Peering conection 

-- go to VPC and check in peering and do accept 

-- once u accept , open private RT and dd route of VPC trafic of ireland (192.169.0.0/16) 

-- do same i ireland private RT (192.168.0.0/16) 


-- now go to SG of mumbai --> add 192.169.0.0/16 in inbound rules 

-- do vice versa in ireland SG 192.168.0.0/16

-- once u do u can able to login to irland vpc private server 




-------- if u want to create vpc for ur company 

u have to create 

1 Virtual Private Gateway 

2 Customer Gateway 

3 Site-to-Site Vpn --> ownlaod configuration n give it to the comonay network admin to setup VPN Connction from Company to AWS 







==============================AMAZON ATHENA========================================


-- it is Quering tool , Start Quering Data instantly . Get Result in seconds , pay only for the queries that u run 

-- Athena is an Interactive Query service that makes it easy to analyze data in A S3 using "Standard SQL"

-- it is serveless , so there is no nfrastructure to manage , and u can pay only for the queries that u run 


-- just point to ur data in A S3 , define the SCHEMA and start Quering using STD SQL 

-- Most results delivered within seconds , with Athena , there is no need for complex ETL Jobs to prepare ur data for analysis 

-- This makes it easy for nayone with SQL Skills to quickly analyze large -scale databasets


-------------------Benefits 


-- Start quering instantly 

-- pay per query ( 5 dollads per TB scan) 

-- open, powerful, standard

-- it is really Fast 


------------------------How it works ? 

see in amazoon website documentation 




=================================================What is Amazon CloudSearch


-- it is Managed Service in the AWS Cloud 

-- it makes it simple and cost-effective to setup, manage and scale a search solution for ur website or appn 

-- Supports 34 languages

-- popular Search features such as 

- highliting 

- autocomplete and 

- Geospatial search 


---------------Benefits 

- simple 

- Auto-Scalable 

- Reliable 

- High Performnace 

- Fully Managed 

- Cost-Effective and Secure 





=========================================Connect to AWS EC2 Using AWS SSM Session Manager ,Secure your EC2 by Enabling AWS SSM===========================


-- Why we need this ?


Let us assume that u have ec2 in a Private Subnet and need to connect to the instance without using SSH over the internet . How will u do it?


Ans : by using  "Using AWS SSM Session Manager"

for this u have to create IAM Role for ec2 instance and attach "AmazonSSMManagedInstanceCore" Policy 


-- U no need to worry about following points 

1 no Ports arre needed to be allowed in SG 

2 U can run instances in Private subnets 

3 there is no need of SSH keys 

4 u can delegate access to manage c2 insatcnes using IAM roles.



Note: By default there are few AMI's that have SSM installed already , it is like a C.W Agent , but it is to be installed 


--- some AMI's that have SSM Agent pre-installed

Amazon Linux Base AMIs dated 2017.09 and later

Amazon Linux 2

Amazon Linux 2 ECS-Optimized Base AMIs

Amazon Linux 2023 (AL2023)

Amazon EKS-Optimized Amazon Linux AMIs

macOS 10.14.x (Mojave), 10.15.x (Catalina), 11.x (Big Sur), and 12.x (Monterey)

SUSE Linux Enterprise Server (SLES) 12 and 15

Ubuntu Server 16.04, 18.04, 20.04, and 22.04

Windows Server 2008-2012 R2 AMIs published in November 2016 or later

Windows Server 2016, 2019, and 2022




to check the status 

-- Amazon Linux        

sudo status amazon-ssm-agent


--  Amazon Linux 2 and Amazon Linux 2023         

sudo systemctl status amazon-ssm-agent


--   SUSE Linux Enterprise Server       

sudo systemctl status amazon-ssm-agent           



-----------------------------DEMO


-- check for linux 2 machine , o need to give keys n SG optional 


-- now we want to connect thruh SSM session but we are not getting option to get connect through the SSM sesion 

-- to do that we know it has aleady pre -installed SSM Agent , verify it is there or not by connecting through ec2-connect 

sudo systemctl status amazon-ssm-agent

it is running 


-- now u have to create role for SSM Session 

IAM --> roles --> creatae role for EC2 --> attach AmazonSSMManagedInstanceCore policy -->create role --> attach this role to the Ec2 instance 

--it will take 5-10 min to connect through SSM session 




=======================Automatically start and stop ur instance with instance scheduler=========================================

-- launch the instance schedular task hub

-- when u click on launch solutions , it will redirect to the cloud formation it takes url for s3 automatically 

-- leave all the values default , in last step enable checkbox

-- do submit the stack it will apporx five minutes to complete the stack 

-- here automatically it will create dynamodb table , lambda function, IAM , SNS, setup KMS and other things as shown in the diagram 

-- once u done ,now configure Periods and schedules 

-- basically schedule is made up of multiple periods

-- The Instance Scheduler on AWS allows you to automatically start and stop Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Relational Database Service (Amazon RDS) instances.some example schedules that can be adapted to many common use-cases.

-- sample schedules

1  Standard 9-5 working hours

this schedule shows how to run instances on weekdays from 9 AM to 5 PM 

Periods

This period will start instances at 9 AM and stop instances at 5 PM on weekdays (Mon-Fri).


Field	      Value

begintime	09:00
endtime	        16:59
name	        weekdays 9-5
weekdays	mon-fri


Schedule

The schedule combines the three periods into the schedule for tagged instances. The schedule includes the following fields and values.

Field	     Value

name	      london-working-hours
periods	      weekdays 9-5
timezone.     Europe/London


2. Stop instances after 5 PM


Instances can be started freely at any time during the day and this schedule will ensure that a stop command is automatically sent to them at 5 PM ET every day.


Periods

This period will stop instances at 5 PM every day.

Field	Value

endtime	16:59
name	stop-at-5 

Schedule

The schedule name provides the tag value that must be applied to insances and the timezone that will be used.

Field	        Value

name	        stop-at-5-new-york
periods	        stop-at-5
timezone	America/New_York


3. Stop instances over the weekend

This schedule shows how to run instances from Monday 9 AM ET to Friday 5 PM ET. Since Monday and Friday are not full days, this schedule includes three periods to accommodate: Monday, Tuesday-Thursday, and Friday.

Periods

The first period starts tagged instances at 9 AM Monday and stops at midnight. This period includes the following fields and values.


Field	    Value

begintime.   09:00
endtime	     23:59
name	     mon-start-9am
weekdays.    mon

The second period runs tagged instances all day Tuesday through Thursday. This period includes the following fields and values.

Field	       Value

name	       tue-thu-full-day
weekdays	tue-thu


The third period stops tagged instances at 5 PM on Friday. This period includes the following fields and values.

Field	    Value

begintime   00:00
endtime	    16:59
name	    fri-stop-5pm
weekdays    fri

Schedule

The schedule combines the three periods into the schedule for tagged instances. The schedule includes the following fields and values.


Field	  Value  

name	  mon-9am-fri-5pm
periods	  mon-start-9am,tue-thu-full-day,fri-stop-5pm
timezone  America/New_York



-- When deploying the Instance Scheduler Hub Stack, the solution created an Amazon DynamoDB table containing several sample periods and schedules that you can use as a reference to create your own custom periods and schedules. To create a schedule in DynamoDB, modify one of the schedules in the configuration table (ConfigTable) or create a new one.

-- open dynamodb table , cf will create tables for us , go n check tables 

-- for this example , I have taken 

Period = office-hours

Schedule = uk-office-hours

IMp : name of the period is to be same in the schedule period column

-- change time zone in the schedule time zone 

-- go to instance --> Tags 

-- here tags are very important to do the actions so be careful while giving the tags 

Here       key = Schedule      value = Schedule's Name 

Eg:               Schedule             uk-office-hours



-- once u set all these values u will able to see the instance will get stopped . By the specific time we have given 

-- done 




============================================================AMAZON SQS========================================

Amazon SQS provides queues for high-throughput, system-to-system messaging. You can use queues to decouple heavyweight processes and to buffer and batch work. Amazon SQS stores messages until microservices and serverless applications process them.

-- it is a message queuing service

How it works ?

workflow 

producers --> AWS SQS --> SQS Queue --> Consumers


-- Amazon SQS allows producers to send messages to a queue. Messages are then stored in an SQS Queue. When consumers are ready to process new messages they poll them from the queue. Applications, microservices, and multiple AWS services can take the role of producers or consumers.


Benefits and features : 

1 Highly scalable Standard and FIFO queues

Queues scale elastically with your application. Nearly unlimited throughput and no limit to the number of messages per queue in Standard queues. First-In-First-Out delivery and exactly once processing in FIFO queues.

2 Durability and availability

Your queues are distributed on multiple servers. Redundant infrastructure provides highly concurrent access to messages.

3 Security

Protection in transit and at rest. Transmit sensitive data in encrypted queues. Send messages in a Virtual Private Cloud.

4 Batching

Send, receive, or delete messages in batches of up to 10 messages or 256KB to save costs.


-- it is the first service launched by the AWS in 2004 , it is very popular for the de-coupling the applications 



--------- it has 2 components Standard and FIFO queues , the differences are 

1  Message Order

-- Standard queues provide best-effort ordering which ensures that messages are generally delivered in the same order as they are sent. Occasionally (because of the highly-distributed architecture that allows high throughput), more than one copy of a message might be delivered out of order.

-- FIFO queues offer first-in-first-out delivery and exactly-once processing: the order in which messages are sent and received is strictly preserved.

2  Delivery

-- Standard queues guarantee that a message is delivered at least once and duplicates can be introduced into the queue.

-- FIFO queues ensure a message is delivered exactly once and remains available until a consumer processes and deletes it; duplicates are not introduced into the queue.

3  Transactions Per Second (TPS)

-- Standard queues allow nearly-unlimited number of transactions per second.

-- FIFO queues allow to process up to 3000 messages per second per API action.

4  Regions

-- Standard queues are available in all the regions.

-- FIFO queues are currently available in limited regions only.

5  AWS Services Supported

-- Standard Queues are supported by all AWS services.

-- FIFO Queues are currently not supported by all AWS services like: CloudWatch Events, S3 Event Notifications, SNS Topic Subscriptions, Auto Scaling Lifecycle Hooks, AWS IoT Rule Actions, AWS Lambda Dead Letter Queues.


What is Visibility timeout ?

-- Visibility timeout sets the length of time that a message received from a queue (by one consumer) will not be visible to the other message consumers.

-- The visibility timeout begins when Amazon SQS returns a message. If the consumer fails to process and delete the message before the visibility timeout expires, the message becomes visible to other consumers. If a message must be received only once, your consumer must delete it within the duration of the visibility timeout.

-- The default visibility timeout setting is 30 seconds. This setting applies to all messages in the queue. Typically, you should set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.

eg : if one consumer is request a meaasge from SQS queue it will give message and it has 30 sec default value to read and deletes it , now if consumer 2 has requested for same message it won't send 'coz the visibility timeout is in active for 30 sec , so after 30 sec it will give message to other consumer 

consumer will be : Applications , lambda functions, ec2 instances, and other aws services 

-- If standard queue, when one consumer pick a message and fails, it will be available for other consumers within the visibility period.

-- FIFO : If a message must be received only once, your consumer must delete it within the duration of the visibility timeout

what is Delivery delay(DelaySeconds)?

 Any messages that you send to the queue remain invisible to consumers for the duration of the delay period.

 -- Receive message wait time(ReceiveMessageWaitTimeSeconds): the maximum amount of time that polling will wait for messages to become available.

 -- Message retention period(MessageRetentionPeriod): amount of time that Amazon SQS retains a message that does not get deleted.

 -- Maximum number of receives per message(maxReceiveCount): If the ReceiveCount for a message exceeds the maximum receive count for the queue, Amazon SQS moves the message to the associated DLQ.

----------------------------------
-- using lambda and SQS combination it is very powerful event driven solutions so, lambda polls SQS and process the messages and these messages are read in the batches and lambda function is invoked once for the each batch so u define the batch size in the lambda configuration , u can specify the batch size b/w 1 - 1000 and default value for the batch  10 

lambda configuration for SQS

 1 SQS Batch processing : 

-- As we discussed SQS messagaes are processed in batches not individually 

-- Lambda deletes the SQS batch  automatically , if all messages e process successfully to avoid re-processing

Failed scrnarios:

-- if one or more messages will fail  lambda ll re-process the batch and u may get duplicate records (that's why Idempotency  important in lambda)

How to avoid re-processing messagaes

-- Reporting batch items Failures feature provided by the AWS 

-- only failure messages will re-process through Reporting batch items Failures


2  Batch Size :

-- messages u want to process  once , default value is 10 

-- keep the batch size 1 as small as , coz lambda will encounter lot of messages , it will keep on creating new lambda instance, u might face throttling issues so it has some limit to create lambda functions ryt? so 

-- it is very important aspect of lambda-SQS integration 

3 Batch Window :

-- time u want to wait before invoking the function. it has to do with SQS Polling 
-- default value is 0 . Which means the lambda will keep on polling SQS to know if it has messages equals o batchsize 

-- Important as it can lead to un-necessary lambda polling hence un-necessary SQS and lambda billing


Handling Failed messages (recommended)

-- if a message s failed, it is retried by lambda until 

- retention period is reached (14 days)
- there is a dead-letter queue 

-- so create DLQ with SQS so that ur lambda does not keep on invoking for 14 days 

eg : ur retried policy number is 5 , it will picked by lambda only for 5 times after 5 times tyhe lambda will send this message to DLQ , this will saves u costs , un-necessary cloud watch logs ....


==================LAB=================

AWS SQS Trigger To Lambda Function


step 1 : create one lambda function

-- choose blue print --> search for SQS select that --> name of ur function --> create new role --> create function 

-- once u create function with the new role created by the lambda function , u have to add other policies like  "AmazonSQSFullAccess", "AWSLambdaSQSQueueExecutionRole", to get permissions 


Step 2 : create Queue

-- create queue as standard queue --> give name and create queue

-- now whatever the messages that we are giving here that will be going to trigger lambda 

-- so create messages first 

--  open queue --> below c.o lambda triggers add lambda function --> c.o send and receive messages --> once u trigger successfully with lambda type some messages in message box 

-- once u do go n check in cloudwatch logs to see our messages 


-- create one FIFO queue also 

-- select Content-based deduplication in queue settings 

-- when u try to enter message it will send to lambda 

-- now try to send same message , it wont trigger in lambda 'coz here deduplication is going on through the FIFO , it wont send duplicate data again ,

-- try to change the content , now u will see 

or u can also so with group level deduplication also 

-- the FIFO queue must end with the .fifo only 

======================================================AMAZON EVENTBRIDGE============================================


-- Amazon EventBridge is a serverless service that uses events to connect application components together, making it easier for developers to build scalable event-driven applications.

-- Event Bridge is an Event Bus that helps in integrating different AWS Services , and custom applications and SAAS Applications 

-- Earlier we have Cloud watch events , the only negative point is that , it did not support SAAS applications and custom applications outside from the AWS 

-- EventBridge was build for the same purpose , u can create effortless event-driven architectures

Eg 1 : u want to receive SNS notification every time production EC2 instance are terminated , this will be done by EventBridge

Eg 2 : automated deployment using code pipelines at 11PM everyday

Eg 3 : if a premier user deactivates himself from ur product , u want to :

- Send and email for feedback 
- Schedule Customer Representative call
- Send customised offers for that use 


-- these all things will done by the event Bridge and u can schedule the events also 

-- It is fully managed service 

-- pay what u use model 


Different Parts in EventBridge:


Working flow -----


Event Producer --> Event --> Amazon EventBridge event Bus --> Rule ---AWS Lambda , AWS Kinesis Data Firehose, Amazon Simple Notification Service


--  Event producer : AWS Service / Custom applications / Third Party SaaS providers,

--  From the producers the event will generated , this will transform to the bus and it move to rules , in rules there are multiple rules and this rules will send to multiple targets 

-- A single rule may have multiple targets and max target is 5 ,all targets will process in the event in parallel


Important terms in EventBridge :

1 Event  : An Event indicated a change in an environment 

2 Rule   : A rule matches incoming events and routes them to targets for processing  

3 Target : target application of ur rule. A target process events 

Targets can include ec2 , lambda functions , kinesis streams , Ecs tasks,  , the target receive events in JSON format 

4 Event Bus : An event bus receives events. When u create a rule , u associate it with a specific event bus and rules is matched only to events received by that event bus 

- rule can not be create standAlone , it should have it's parent as EventBus

Components of EventBridge :

1 EventBridge Rule : A rule matches incoming events and sends them to targets for processing.

2 EventBridge Pipes : A pipe connects an event source to a target with optional filtering and enrichment.

3 EventBridge Schedule : A schedule invokes a target one-time or at regular intervals defined by a cron or rate expression.

4 EventBridge Schema registry : Schema registries collect and organize schemas.


EventBus : Usecases

if a premier user deactivates himself from ur product , u want to :

- Send and email for feedback 
- Schedule Customer Representative call
- Send customised offers for that use 


In the above example , the working flow will be like 


User deletes his subscription --> custom event is trigged --> rights custom event would be pass into the EventBus --> rule matching --> then it reaches to the targets , once the rule is matched  here then it sends to targets --> 1 the targets are Feedback service 2 Schedule call service 3 offers service 




=======================================practicals=======================


Example 1 

1  Schedule AWS Lambda Functions Using EventBridge


-- lambda scheduling use cases 

- Automated backups at EOD of ur applications 

- backend cleaning (including logs and temp files )

- consolidated reports after business hours , so lambda can trigger  Athena queries and can run any database queries and send to business stack holders through SNS 


- to schedule events u have two options 

1 to schedule  at fixed rate -- for every 1 minute 

2 Cron job (10 * * * * )

-- open console 

-- open lambda --> python 3.9 --> create function 

-- write python code to invoke lambda function

import json
from datetime import datetime

def lambda_handler(event, context):
    # TODO implement
    currentTime = datetime.now()
    print("Time at which Lambda invoked" + str(currentTime))


-- give empty JSON to test {}

-- now go to Amazon eventBridge in console 

-- c.o create rule --> schedule --> continue to create rule --> schedule that run at regular intervals--> select 1 minute --> select lambda function --> create rule 

-- u can check in CloudWatch logs , for invoking is there or not 

--  the logs are generated 


-- if u want to monitor ur invocations --> Metrics --> all Metrics --> query --> choose AWS/events --> Myrule name 

- metric name = COUNT(invocation)

- filter by = Rulename = rule name that u have created in lambda function to test the function 

- choose number in right corner to see the no. of invocations 


-------------------------------------------

Example 2 : EventBridge with SNS 


-- AS we know that EventBridge will use for 2 process 

1 event-event process : when the event has occurred according to our rule then it will trigger to the targets 

2 schedule process : do schedule 

-- in the above example we have seen Schedule event

-- now event to event 

- for example the ec2 is stopped then it will send alert to the subscribers on SNS Topic 

-- open SNS in console --> create one topic --> standard --> create topic --> open topic -->create subscription --> add protocol email or phone number 

-- when ever the instance is getting stooped then I would like send an alert to all the subscribers 

-- this is called event-event process 

-- create one ec2 instance 

-- create rule in event bridge --> name --> rule with an event pattern --> in event pattern = select ec2 --> select event type as u want --> in Target1 choose AWS Service = SNS topic select SNS topic --> create rule 

-- do stop the instance 

-- u will get notification once the instance is get stopped 


--------------- now for schedule event 

-- create another rule , that is based on the time to stop the instance 

-- select schedule pattern option --> A schedule that runs at a regular rate, such as every 10 minutes. --> target = terminateinstanceAPI call --> u can also add another target to get notification 
 

-- the instance will get terminated after the time that u have specified 


-- this is simple basic example for schedule pattern 



==========================done=============================



=====================================================================AMAZON ATHENA=============================================

what is AWS Athena ?

ANS : -- Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.

-- Athena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds. With Athena, there’s no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.

-- Athena is out-of-the-box integrated with Amazon Glue Data Catalog, allowing you to create a unified metadata repository across various services, crawl data sources to discover schemas and populate your Catalog with new and modified table and partition definitions, and maintain schema versioning. You can also use Glue’s fully-managed ETL capabilities to transform data or convert it into columnar formats to optimize cost and improve performance.

Benefits :
 
1  Interactive Performance Even for Large Datasets :

With Amazon Athena, you don't have to worry about having enough compute resources to get fast, interactive query performance. Amazon Athena automatically executes queries in parallel, so most results come back within seconds.

2  Built on Presto, Runs Standard SQL

Amazon Athena uses Presto with ANSI SQL support and works with a variety of standard data formats, including CSV, JSON, ORC, Avro, and Parquet. Athena is ideal for quick, ad-hoc querying but it can also handle complex analysis, including large joins, window functions, and arrays. Amazon Athena is highly available; and executes queries using compute resources across multiple facilities and multiple devices in each facility. Amazon Athena uses Amazon S3 as its underlying data store, making your data highly available and durable.

3  Serverless, No ETL

Athena is serverless. You can quickly query your data without having to setup and manage any servers or data warehouses. Just point to your data in Amazon S3, define the schema, and start querying using the built-in query editor. Amazon Athena allows you to tap into all your data in S3 without the need to set up complex processes to extract, transform, and load the data (ETL).

4  Only Pay for Data Scanned

With Amazon Athena, you pay only for the queries that you run. You are charged ¥41.20 per terabyte scanned by your queries. You can save from 30% to 90% on your per-query costs and get better performance by compressing, partitioning, and converting your data into columnar formats. Athena queries data directly in Amazon S3. There are no additional storage charges beyond S3.


Features :

1  Fast Performance

With Amazon Athena, you don’t have to worry about managing or tuning clusters to get fast performance. Athena is optimized for fast performance with Amazon S3. Athena automatically executes queries in parallel, so that you get query results in seconds, even on large datasets.

2  Easy to Get Started

To get started, log into the Athena console, define your schema using the console wizard or by entering DDL statements, and immediately start querying using the built-in query editor. You can also use Amazon Glue to automatically crawl data sources to discover data and populate your Data Catalog with new and modified table and partition definitions. Results are displayed in the console within seconds, and automatically written to a location of your choice in S3. You can also download them to your desktop. With Athena, there’s no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.

3  Serverless, Zero Infrastructure, Zero Administration

Amazon Athena is serverless, so there is no infrastructure to manage. You don’t need to worry about configuration, software updates, failures or scaling your infrastructure as your datasets and number of users grow. Athena automatically takes care of all of this for you, so you can focus on the data, not the infrastructure.

4  Pay per Query

With Amazon Athena, you pay only for the queries that you run. You are charged based on the amount of data scanned by each query. You can get significant cost savings and performance gains by compressing, partitioning, or converting your data to a columnar format, because each of those operations reduces the amount of data that Athena needs to scan to execute a query.

5  Easy to Query, Just Use Standard SQL

Amazon Athena uses Presto, an open source, distributed SQL query engine optimized for low latency, ad hoc analysis of data. This means you can run queries against large datasets in Amazon S3 using ANSI SQL, with full support for large joins, window functions, and arrays. Athena supports a wide variety of data formats such as CSV, JSON, ORC, Avro, or Parquet. You can also connect to Athena from a wide variety of BI tools using Athena's JDBC driver.

6  Highly Available & Durable

Amazon Athena is highly available and executes queries using compute resources across multiple facilities, automatically routing queries appropriately if a particular facility is unreachable. Athena uses Amazon S3 as its underlying data store, making your data highly available and durable. Amazon S3 provides durable infrastructure to store important data and is designed for durability of 99.999999999% of objects. Your data is redundantly stored across multiple facilities and multiple devices in each facility.

7  Secure

Amazon Athena allows you to control access to your data by using Amazon Identity and Access Management (IAM) policies, access control lists (ACLs), and Amazon S3 bucket policies. With IAM policies, you can grant IAM users fine-grained control to your S3 buckets. By controlling access to data in S3, you can restrict users from querying it using Athena. Athena also allows you to easily query encrypted data stored in Amazon S3 and write encrypted results back to your S3 bucket. Both, server-side encryption and client-side encryption are supported.


Different ways to access Athena :

1 AWS console
2 Athena API 
3 Athena CLI 
4 JDBC connection

-- it integrates with AWS Glue Data Catalog , it is ETL Tool

-- it also integrates with Amazon Quicksight for data visualization 


Why Athena got popular:

-- used by data Analysts  query large S3 data statements

-- No need to spin up serveres or Hadoop clusters

 Athena Use cases :

-- analyze CloudTrail/CloudFront/VPC/ELB logs
-- integration trough ODBC/JDBC with other visualization tools
-- Ad-hoc logs analysis

Athena Cost Model : 

-- pay only for queries

- $5 per TB data Scanned
- charged for the no.of bytes scanned (with min 10MB PER Query)
- NO charge for DDL(CREATE,ALTER,DROP) and failed quaries
- charges for cancelled queries (for the data scanned)

-- How to save cost?

- Columnar formats(Parquet and ORC)
- using partitions
- compressions(gzip,snappy)
- number of files(more related performance optimization) : Size of files stored in the s3 , it should be min 10MB to 50MB then Athena should not spend effort in scanning multiple small files 


-- Athena - creating tablesm

-- create table using 

- DDL Commands
- AWS Glue crawler
- JDBC driver
- create table wizard -- through AWS console

Note : only EXTERNAL tables re created in the Athena if u dont provides EXTERNAL keyword , u will get error

-- Athena queries

- quer results and metadata stored in s3 
- u specify the specific folder in s3 to store the output
- stored procedures not supported
- certain DDL Operations are also not supported
- Athena federated query (preview feature) : when u query the data , u provide s3 folder so, with federated queries u can combine the results from multiple data eg dynamoDB, AWS storage service,


---------------Practicals---------------------

Step 1 : create s3 bucket and insert some .csv data 

--  insert some  data in the bucket 

--   create one folder inside bucket and go to google and search for sample .csv files downlaod and upload in the s3 bucket 

Step 2 : Glue

-- open glue in console

-- glue --> left panel data Catalog --> crawlers --> add crawlers --> give name of ur crawler --> not yet --> add a data source --> choose s3 --> copy uri of ur bucket and paste in S3 path --> click on create new role , give name of ur role and select nxt --> click on create new database --> give name of ur database , nxt --> frequency = on-demand --> create crawler

-- now try to run the crawler , 

--  u can also check the logs in cloudwatch 

-- once u done with the running part , a table will added to the database , table name is same as the folder name of s3 that u have uploaded 

--  if ur data is not match , then u can also do edit ur schema 


step 3 : Athena

-- open Athena in the console

-- in the left panel u will see the database and table , u can check all the coloums of ur table

-- now do queries , before that u should add destination to store our outputs in the s3 bucket 

-- go to s3 buckets create one folder in the bucket to store the outputs

-- now try to query in athena editor

-- SELECT * FROM "weather-database"."weather_csv" limit 10;

above is eg query , u can replace with ur details 

weather-database = database name 

weather_csv = table name


NOTE : if u r getting zero records but query is successfull then do 

ur s3 location is like this 

s3://doc-example-bucket/table1.csv 

-- to get avoid from this error , jst create one sub-folder and upload .csv file in that subfolder 

s3://doc-example-bucket/table1/table1.csv



-- in the crawler location u have to give like this , then u won't get any error

s3://doc-example-bucket/table1/



=============================================================DynamoDB Streams=================================================

DynamoDb Streams : it is an ordered flow of information anout changes  items in a DynamoDb table. When u enable stream on a table , DynamoDb captures information about every modification to data items n the table 

-- Capture item-level changes in your table, and push the changes to a DynamoDB stream. You then can access the change information through the DynamoDB Streams API.

Stream Records can be used for 

-- Kinesis Data Streams/Client library 
-- AWS Lambda
--Data is retined for 24 hrs 


Uses For : 

-- Analytics purpose
-- Cross Region replication
-- Real time changes

Step 1 : create DynamoDb table

-- create table and give partiton key

-- once u create table , add some items into the table

-- go to table --> exports and streams --> turn on DynamoDB stream --> New and old images --> Turn on streams 


Step 2 : create lambda function 

--  in the same page , choose create trigger

-- create new function --> Use Blue print --> search for DynamoDB and select Process updates made to table with python --> give function name --> choose create a new role --> give DynamoDb details in trigger section --> create function

-- once ur function got created , u will get some error 

--  function --> configuration --> permissions --> c.o role --> attach permissions (policies) --> DynamoDBfullaccess --> do refresh the screen the error will disappear

-- ince ur  function is created go back to the trigger page in DynamoDB table add this function with batch size is 100 for this eg 


Step 3 : Test 

-- go back to the DynamoDB table and update and add some items into the table 

-- do some chganges as you want 

-- the changes will trigger to our lambda function '

-- now go to function --> monitor--> view cloudwatch logs --> now you will able to see all the changes , INSERT,UPDATE,DELETE all the modifications that are captured through the DynamoDB Stream APi and trigger through the lambda and you will able to see in the cloud watch logs what has modification occurs 



=======================================DONE===========================================


==================================================S3 Batch Replication===============================




NOTE : Do not create your own IAM Roles and policies for this project , You just go with "create new Role" option while doing this process



Replicate Existing Objects in your Amazon S3 Buckets with Amazon S3 Batch replication


Mian Aim is how to replicate objects already existing in your buckets within the same AWS Region or across different AWS Regions with Amazon Simple Storage Service (Amazon S3) Batch Replication.


Amazon S3 Replication is an elastic, fully managed, low-cost feature that replicates objects between Amazon S3 buckets. You can replicate new and existing data from one source bucket to multiple destination buckets in the same or different AWS Regions. Whether you want to maintain a secondary copy of your data for data protection or have data in multiple geographies to provide users with the lowest latency, S3 Replication gives you the controls you need to meet your business needs.


You can use Amazon S3 Batch Replication to backfill a newly created bucket with existing objects, re-replicate objects that have replicated already or were previously unable to replicate, migrate data across accounts, or add new buckets to your data lake. S3 Batch Replication jobs are created on top of an existing replication configuration and run for all replication rules that are enabled for the bucket. For more information on S3 Replication, visit the Replicating objects section in the Amazon S3 User Guide, and for a step-by-step tutorial on setting up S3 Replication visit Replicate data within and between AWS Regions using Amazon S3 Replication. By the end of this tutorial, you will be able to replicate existing data within and between AWS Regions using Amazon S3 Replication.


Data lakes : Data lakes on AWS help you break down data silos to maximize end-to-end data insights. With Amazon Simple Storage Service (Amazon S3) as your data lake foundation, you can tap into AWS analytics services to support data your needs from data ingestion, movement, and storage to big data analytics, streaming analytics, business intelligence, machine learning (ML), and more – all with the best price performance. More than 700,000 data lakes run on AWS.

Amazon S3 is the best place to build data lakes because of its unmatched durability, availability, scalability, security, compliance, and audit capabilities. With AWS Lake Formation, you can build secure data lakes in days instead of months. AWS Glue then allows seamless data movement between data lakes and your purpose-built data and analytics services.

Amazon Glue : 

AWS Glue is a serverless data integration service that makes it easy for analytics users to discover, prepare, move, and integrate data from multiple sources. You can use it for analytics, machine learning, and application development.


Configure S3 Replication on your Amazon S3 bucket :

-- Create two S3 buckets

-- Create an S3 Replication rule on your S3 bucket

-- Choose a destination S3 bucket

-- Choose or create IAM roles for replication

-- Specify the encryption type (optional)

-- Choose the destination S3 storage class

-- Enable additional replication options (optional)


Configure S3 Batch Replication for existing objects in your Amazon S3 bucket in the following ways:

-- Create an S3 Batch Replication job when you create a new replication configuration on your bucket or when you add a new destination to your existing replication configuration

-- Create an S3 Batch Replication job from the S3 Batch Operations home page (recommended)

-- Create an S3 Batch Replication job from existing replication configuration page


Implementation :

Step 1: Create two Amazon S3 buckets

-- Log in to the AWS Management Console using your account information. In the search bar, enter S3, then select S3 from the results.

-- In the left navigation pane on the S3 console, choose Buckets, and then choose Create bucket.

-- Enter a descriptive, globally unique name for your source bucket. Select the AWS Region you want your bucket created in. 

-- Enable Bucket Versioning. Bucket versioning is required for both source and destination S3 buckets for S3 Replication. 

-- You can leave the remaining options as defaults. Navigate to the bottom of the page and choose Create bucket.

-- Repeat the preceding steps to create another S3 bucket to serve as the destination bucket. This new bucket can exist in the same AWS Region as the source bucket for S3 Same-Region Replication (S3 SRR) or in a different AWS Region for S3 Cross-Region Replication (S3 CRR). Make sure to enable Bucket Versioning for the destination S3 bucket and name your new bucket something unique.


Step 2: Create an S3 Replication Configuration on your S3 bucket

-- From your list of S3 buckets, choose your source S3 bucket. The console takes you to the S3 bucket landing page.

-- On the S3 bucket landing page, you can review the Objects, Properties, Permissions, Metrics, Management, and Access Points for the selected S3 bucket.

-- On the Management tab, under Replication rules, select Create replication rule.

-- Enter a Replication rule name and make sure Enabled is selected under the Status section. If the replication rule is disabled, it will not run.

NOTE: Amazon S3 attempts to replicate objects according to all replication rules. However, if there are two or more rules with the same destination bucket, then objects are replicated according to the rule with the highest priority. The lower the number, the higher the priority. You can edit the priority of each replication rule on the replication configuration page.  

-- Narrow the scope of replication by defining a Filter type (Prefix or Tags), or choose to replicate the entire bucket. For example, if you want to only replicate objects that include the prefix Finance, specify that scope. For more information on filtering objects for replication

--  choose  rule scope      Limit the scope of this rule using one or more filters

-- Limit the scope of this rule using one or more filters

-- You can't create a new S3 bucket during the replication setup process.

-- Encryption is optional

-- Destination storage class is optional 

-- Choose any additional replication options you need:

-- Replication Time Control (RTC): S3 RTC helps you meet compliance and business requirements because it provides an SLA of 15 minutes to replicate 99.99% of your objects. You can enable S3 RTC along with S3 CRR and S3 SRR. Replication metrics and notifications are enabled by default.

-- Replication metrics and notifications: For non-RTC rules, you have the option to select Replication metrics and notifications, which provides detailed metrics to track minute-by-minute progress of bytes pending, operations pending, operations failed, and replication latency for the replication rule.

-- Delete marker replication: Selecting Delete marker replication means deletes on the source bucket will be replicated to the destination bucket. This should be enabled if you want to keep the source and destination buckets in sync, but not if the goal is to protect against accidental or malicious deletes.

-- Replica modifications sync: To establish two-way replication between two S3 buckets, create bidirectional replication rules (A to B, and B to A) and enable Replica modification sync for the replication rules in both the source and destination S3 buckets. This will help you to keep object metadata such as tags, ACLs, and Object Lock settings in sync between replicas and source objects.

NOTE : S3 RTC, Replication metrics and notifications, and Replica modification sync are not supported while replicating existing objects with S3 Batch Replication.

-- When you have configured the replication, choose Save.  

-- When you create the first rule in a new replication configuration for your S3 bucket or add a new destination AWS Region to an existing configuration, you have the option to enable existing object replication for that replication rule. To replicate existing objects, choose, Yes, replicate existing objects, and then choose Submit.

-- The console takes you to the Create Batch Operations job page.



Step 3: Replicate existing objects while creating a new replication configuration


-- On the Create Batch Operations job page, you can review S3 Batch Operations Job settings such as job run options, scope of S3 completion reports, and permissions.

-- Set Job run options. If you want the S3 Batch Replication job to run immediately, you can choose Automatically run the job when it’s ready. If you want to wait to run the job when it’s ready, you can save the Batch Operations manifest to review the list of objects to be replicated.

-- As long as S3 Batch Operations successfully processes at least one object, Amazon S3 generates a completion report after the batch replication job completes, fails, or is cancelled. The completion report contains additional information for each task, including the object key name and version, status, error codes, and descriptions of any errors. We recommend choosing Generate completion report for All tasks so that you can review the status of all objects replicating with this job. For examples of completion reports, 

-- Make sure that the IAM role associated with this Batch Replication job has sufficient permissions to perform S3 Batch Operations on your behalf. For more information, see the documentation on configuring IAM policies for Batch Replication and granting permissions for Amazon S3 Batch Operations.

-- Review the configuration, and select Save.

-- You are redirected to the Batch Operations home page.

-- Select the Job ID of your new job to review the job configuration. You can also track the status of the Batch Replication job.


Step 4: Replicate existing objects with existing replication configuration

-- Apart from creating a Replication job for a new replication rule as described in the previous step, you can also create S3 Batch Replication jobs for existing replication rules in S3 buckets. To do this, return to the Amazon S3 console home page.

-- On the left navigation pane of the console home page, choose Batch Operations, and then choose Create job.

-- On the Create job page, select the AWS Region where you want to create your Batch Replication job. You must create the job in the same AWS Region in which the source S3 bucket is located.

-- Provide the list of objects to replicate. You can add a user-generated manifest in the form of an Amazon S3 inventory report or a CSV file. The manifest needs to have all of the object versions that need to be replicated. Amazon S3 can also generate a manifest for you using the existing S3 Replication configuration on the source bucket.

-- NOTE: In this example, we chose Create manifest using S3 Replication configuration to let Amazon S3 generate a manifest on our behalf and chose “aws-s3-replication-tutorial-source-bucket” as the source bucket. If you choose to let Amazon S3 generate a manifest for you, you will also see additional filters such as object creation date and replication status to reduce the scope of the job.

-- If you chose Create manifest using S3 Replication Configuration on the previous page, the only Operation option is Replicate. This is because replication is the only operation that is permitted while using an S3 generated manifest. Select Replicate, and then choose Next.

-- Enter a Description to best define the purpose of the job.

-- Select a Priority to indicate the relative priority of this job to others running in your account. A higher number indicates higher priority. For example, a job with Priority 2 will be prioritized over a job with priority 1. S3 Batch Operations prioritizes jobs according to priority numbers, but strict ordering isn't guaranteed. Therefore, you shouldn't use job priorities to ensure that any one job starts or finishes before any other job. If you need to ensure strict ordering, wait until one job has finished before starting the next.

-- Choose whether you want to generate a completion report. 

-- Choose a valid Batch Operations IAM role to grant Amazon S3 permissions to perform actions on your behalf.

-- You must also attach a Batch Replication IAM policy to the Batch Operations IAM role. To create a valid IAM role and policy see, configuring IAM policies for Batch Replication.

-- Add Job tags to your Batch Replication job, and then choose Next to review your job configuration.

-- On the Review page, choose Edit to make changes, then choose Next to save your changes and return to the Review page.

-- When your job is ready, choose Create job.  

-- After the Batch Replication job is created, Batch Operations processes the manifest. If successful, it will change the job status to Awaiting your confirmation to run. You must confirm the details of the job before it can run.

-- When the job is successful, a banner displays at the top of the Batch Operations page.


Step 5: Create Batch Replication job from S3 Replication configuration page

-- From your list of S3 buckets, choose the S3 bucket that you want to configure as your source for replication.

-- The console takes you to the S3 bucket landing page.

-- Review the Objects, Properties, Permissions, Metrics, Management, and Access Points for the selected S3 bucket.

-- On the Management tab, under Replication rules, select View replication configuration.

-- On the replication configuration home page for your source bucket, choose Create replication job to go to the Create job page for S3 Batch Operations. Repeat the previous steps to create a Batch Replication job from the existing replication configuration.


Step 6: Monitor the progress of an S3 Batch Replication job

-- After a Batch Replication job is created and run, it progresses through a series of statuses. You can track the progress of a Batch Replication job by referring to these statuses on the Batch Operations home page.

For example, a job is in the New state when it is created, moves to the Preparing state when Amazon S3 is processing the manifest and other job parameters, then moves to the Ready state when it is ready to run, Active when it is in progress, and finally Complete when the processing completes. For a full list of job statuses, see Batch Operations job statuses.

-- You can choose to generate a Completion report when you create your Batch Replication job to track the status of object replication. The Completion report is a CSV file that is generated by Amazon S3 after a job completes, fails, or is cancelled as long as at least one task has been successfully invoked with S3 Batch Operations.

-- Additionally, if you have Replication metrics or S3 Replication Time Control (S3 RTC) enabled for your replication rule, you can review the number of failed operations per minute on the Amazon S3 console and Amazon CloudWatch console with the Operations Failed Replication metric. For more information, refer to S3 Batch Operations completion reports and monitoring progress with S3 Replication metrics.






-- some scenario based question 

1   u have been tasked with encrypting EBS Volumes that are restored from unencrypted EBS snapshots . What should u do ? 

ANS :  To encrypt EBS volumes restored from unencrypted EBS snapshots , u can copy the snapshots and enable encryption with a Symmetric Customer Master key (CMK) while creating an EBS volume using the snapshot. This ensures that the restored volumes are encrypted .


2. U need to limit the maximum number of request from a single IP address. How can u achieve this? 

Ans :  T0 limit the max number of requests from a single IP address, u can create a  “rate-based” rule in was web Application Firewall (WAF) and set the desired rate limit . This helps protect your applications from excessive traffic from Single IP Address.


3  u want to grant the bucket full access to all upload objects in an S3 bucket . What steps should you take ?

Ans :  To grant the bucket owner full access to all uploaded objects in an s3 bucket , u can create a bucket policy that requires users to set the objects Access control list (ACL) to “BUCKET-OWNER_FULL_CONTROL” , This ensures that the bucket owner has complete control over the objects in the bucket 


4. How to protect accidental deletion or overwrite objects in an S3 bucket ? 

Ans:  To protect objects in an S3 bucket from accidental deletion or overwrite , u can enable versioning and enable multi-factor Authentication(MFA) delete. 

Versioning Keeps multiple versions of an object , and MFA delete requires additional authentication before deleting or overwriting objects , adding an extra layer of protection .


5. U want to access resources on both on-premises and aws using on-premises credentials stored in Active Directory . How can u Accomplish this ?

Ans:  To access resource both , u can set up SAML 2.o-Based federation using a Microsoft Active Directory Federation Service (AD FS). This allows users to authenticate with their on-premises credentials and access AWS resources Securely .


6  u need to secure the sensitive data stored in EBS volumes . What should u do ?

Ans: To do this , u can enable EBS Encryption. This encrypts the data at rest, providing an additional layer of security for ur sensitive information 

7 u want to ensure that the data-in-transit and data-at-rest of an amazon S3 bucket is always encrypted . How can u achieve this?]

Ans:  u can enabled Amazon S3 server-Side Encryption or use Client-Side Encryption . These encryption methods protect ur data from unauthorised access during transmission and storage 


8  u need to secure a web application by allowing multiple domains to serve SSL traffic over the same IP address . What steps should u take ? 

Ans :  u can use AWS Certificate Manager (ACM) to generate an SSL certificate. Associate the certificate to a CLOUDFront Distribution and enable Server Name Indication(SNI) , which allows multiple SSL certificates to be served from a single IP address 


9. U want to control access for several buckets by using a gateway endpoint to allow access to trusted buckets . How can u achieve this?

Ans :  u can create an endpoint policy for the trusted s3 buckets . This policy specifies the permissions and access controls for the buckets accessed through the gateway endpoint 


10  u need to setup Asynchronous data replication to another RDS Db instance hosted in another AWS Region . How can you achieve this?

Ans :  u can create a Read Replica . This allows us to to replicate ur data from the primary RDS DB instance to the replica, ensuring data redundancy and availability 


11.  U require	 a parallel file system for “hot” (frequently accessed) data . What AWS services can fulfil this requirement?

Ans :  u can utilize Amazon Fix for lustre . It is a fully managed , high performance file system optimised for workloads that require fast access to data 


12.  U need to implement synchronous data replication across Availability Zones with automatic failover in Amazon RDS . What should u do ?

Ans :  to achieve synchronous data replication across A>Z with automatic failover in A RDS , u can enable “MULTI-AZ “ deployment. This configuration automatically replicates ur databases synchronously to a standby instance in a different Availability Zone, ensuring high availability and automatic failover.


IMP Note : synchronous : Multi-AZ  deployment 

Asynchronous : read Replicas 



13. U require a storage service to host “cold”( infrequently accessed) data. Which was service fulfilled this requirement?

Ans :  to host infrequently accessed data u can leverage amazon S3 Glacier . It is a low-cost storage service designed for long-term archiving and backups of data that is accessed less frequently 


14. U need to setup a relational databases with a disaster recovery plan having an RPO of 1 second and RTO of less than 1 minute . What should You use ? 

Ans :  u can utilise Amazon Aurora Global database. It provides low-latency global replication, enabling rapid failover and ensuring minimal data loss 


15. U want to monitor databases metrics and receive email notifications when a specific threshold is breached . How can u accomplish this ? 

Ans :  u can create an SNS (simple Notification Service ) topic and add the topic in the cloud watch alarm . This enables us to to receive alerts via email when specific metric thresholds are crossed 


16.  U need to setup DNS failover to a static website . How can you achieve this?

Ans :  us  R53 


17. U want to implement automated backups for all ur EBS volumes . What should u do ? 

Ans:  u can use Amazon data LifeCycle Manager . This service allows u to automate the creation and recreation of EBS snapshots based on customisable schedules and lifecycle policies 

18. U need to monitor the available swap space of ur ec2 instances . How can u monitor this metric ? 

Ans :  to monitor the available swap space of ur ec2 instances , u can install the Cloudwatch Agent and configure it to monitore the Swaputilization metric . This enables us to to track the swap space usage and take appropriate actions if necessary 


19.  How to implement fanout messaging ?

Ans:  to implement this u can create an SNS topics with a message filtering policy and configure multiple SQS queues to subscribe to the topic 


20.  U require a DB that has a read replication latency of less than 1 Second? 

Ans:  u can use Amazon Aurora with cross-region replicas 


21 A specific type LB that use UDP a the protocol for communications b/w clients and thousands of game servers around the world 

Ans : use network load balancer  for TCP/UDP protocols 


22 how to monitor disk and memory space utilisation of an ec2 instance ?

Ans :  install Cloud watch Agent  on instance  


23. Retrieve a subset of data from a large CSV file stored in the s3 bucket ? 

Ans:  perform S3 select operation based on the bucket’s name and object’s key.

S3 select allows u to select specific portion of data from large objects  reducing the amount of data transfer  and also improving performance at the same time 


24.  U have to upload 1TB file to s3 

Ans : use Multi-part upload API to upload large objects in parts 


25.  Improve the performance of the applications by reducing the response times from milliseconds to microseconds ?

Ans:  use  Amazon DynamoDb Accelerator (DAX) , it is in-memory caching service for the Dynamodb , it significantly reduce the database read latency  and improves application performance 


26.  IMP q . Retrieve the instance Id , Public Keys , and Public IP address of an Ec2 instance . 

Ans:  Access the URL 


TOKEN=‘curl -X PUT ”http://169.254.169.254/latest/api/token” -H ”X-aws-ec2-metadata-token-ttl-seconds: 21600”‘ \ && curl -H ”X-aws-ec2- metadata-token: $TOKEN” -v http://169.254.169.254/latest/meta-data/ 

This is also Called magic Ip Address 


27 Route the internet traffic to the resource based on the location of the user ? 
Ans :  use R53 Geolocation Routing Policy 

28.  Suggest A cost -effective soln for over-provisioning of resource ?
Ans:  configure a target Tracking Scaling in ASG , this automatically adjust no.of instances  based on pre-defined metrics optimizing resource allocation and cost management 

29.  The application data is stored in a tape backup soon . The backup data must be preserved for up to 10 years ?
Ans :  Use Was Storage Gateway to backup the data directly to amazon S3 glacier Deep Archive.

30.  Accelerate the transfer of historical records from on-premises to aws over the internet in a cost-effective manner ? 
Ans:  Use Aws Data-sync and select Amazon S3 Deep Archive as the Destination 
Data -sync optimizes the data transfer , ensuring efficient and reliable  to a long term storage 

31. Globally Deliver the static contents and media files to customers around world with low latency ?
Ans: Store the files in Amazon S3 and create Cloud Front Distribution . Select the S3 bucket as the origin 

32 An application must be hosted to 2 ec2 instances and should continuously run for three years . The cpu utilisation of ec2 instances is expected to be stable and predictable?
Ans:  Deploy the application to a Reserved Instance , it provides long-term cost effective options and predictable workloads 

33. Implement a cost -effective soon for s3 objects that are accessed less frequently.
Ans: Create an Amazon life cycle policy to move the objects to amazon s3 Standard-IA 

34.  Minimise the data transfer costs b/w two instances ?
Ans: Deploy the ec2 instance in the same region 

35. Import the SSL/TLS certificate of applications into Aws ?
Ans: Import into Aws Certificate manager or upload it to AWS IAM 







































































